{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Building a Recency-Aware Question-Answering System with Llama-Index\n",
    "\n",
    "\n",
    "In this tutorial, we will learn how to build a question-answering system that is aware of the recency of information. This is especially important for questions whose answers may change over time, such as \"Who is the current US President?\".\n",
    "\n",
    "We will be using the llama_index package, a powerful tool for building large-scale information retrieval systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing necessary packages\n",
    "Before we start, make sure that you have the llama_index package installed. You can install it using pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install llama-index "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will also need the qdrant_client package for this tutorial. You can install it via pip as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install qdrant-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install 'rich[jupyter]'\n",
    "# from rich import print\n",
    "%load_ext rich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import qdrant_client\n",
    "from datetime import datetime\n",
    "from llama_index import GPTVectorStoreIndex\n",
    "\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from pathlib import Path\n",
    "from llama_index import GPTVectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
    "from llama_index.indices.postprocessor import (\n",
    "    FixedRecencyPostprocessor,\n",
    "    EmbeddingRecencyPostprocessor,\n",
    ")\n",
    "# load documents\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "Path.ls = lambda x: list(x.iterdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Documents\n",
    "\n",
    "First, let's load our documents. In this example, we will use the News Category Dataset v3. This dataset contains news articles with various fields like `headline`, `category`, `short_description`, `link`, `authors`, and date. We'll load the data, and reformat it to suit our needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"../data/News_Category_Dataset_v3.json\", \"r\") as f:\n",
    "    data = [json.loads(k) for k in f.readlines()]\n",
    "    links = [k.pop(\"link\") for k in data]\n",
    "    authors = [k.pop(\"authors\") for k in data]\n",
    "    data = [\n",
    "        {\n",
    "            \"text\": f\"{k['headline']} under the category: {k['category']}\\n {k['short_description']}\",\n",
    "            \"date\": k[\"date\"],\n",
    "        }\n",
    "        for k in data\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(datetime.datetime(2012, 1, 28, 0, 0), datetime.datetime(2022, 9, 23, 0, 0))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "dates = [datetime.datetime.fromisoformat(element[\"date\"]) for element in data]\n",
    "min(dates), max(dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll write these documents to text files in a directory. Each document will be written to a text file named by its date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "write_dir = Path(\"../data/dump\").resolve()\n",
    "write_dir.mkdir(exist_ok=True, parents=True)\n",
    "for element in data:\n",
    "    file_path = write_dir / f\"{element['date']}.txt\"\n",
    "    with file_path.open(\"w\") as f:\n",
    "#         print(element[\"text\"])\n",
    "        f.write(element[\"text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what we want to build:\n",
    "\n",
    "This diagram shows that:\n",
    "\n",
    "1. The user issues a query to the query engine.\n",
    "2. The query engine, which has been configured with certain postprocessors, performs a search on the vector store based on the query.\n",
    "3. The query engine then postprocesses the results.\n",
    "4. The postprocessed results are then returned to the user.\n",
    "\n",
    "Each arrow represents the direction of data flow. The \"Query Engine\" box encapsulates the postprocessing step to indicate that it's a part of the query engine's function. This diagram is meant to provide a high-level understanding of the process and does not include all the details involved.\n",
    "\n",
    "\n",
    "We're using Llama Index to make the Query Engine and Qdrant for our Vector Store\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "+-----------+          +------------------+          +-----------------+\n",
    "|           |  Query   |                  |  Search  |                 |\n",
    "|  User     +--------->+  Query Engine    +--------->+     Qdrant      |\n",
    "|           |          |                  |          |   Vector Store  |\n",
    "|           |          | (with            |          |                 |\n",
    "+-----------+          | Postprocessors)  |          +-----------------+\n",
    "                       |                  |          |                 |\n",
    "                       |  +------------+  |          |                 |\n",
    "                       |  |Postprocess |  |  Top K   |                 | \n",
    "                       |  |            |  |Candidates|                 |\n",
    "                       +--|Candidates  |  +<---------+                 |\n",
    "                       |  +------------+  |                            |\n",
    "                       |                  |                            |\n",
    "+-----------+          +------------------+                            |\n",
    "|           |   Final  |                                               |\n",
    "|  User     |<---------+                                               |\n",
    "|           |  Answer  |                                               |\n",
    "+-----------+                                                          |\n",
    "                                                                       |\n",
    "+---------------------------------------------------------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Qdrant Client\n",
    "We'll be using Qdrant as our vector storage system. Qdrant is a high-performance vector database designed for storing and searching large-scale high-dimensional vectors.\n",
    "\n",
    "- You can use `:memory:` mode for fast and lightweight experiments. It does not require Qdrant to be deployed anywhere but requires `qdrant-client >= 1.1.1`.\n",
    "\n",
    "- Otherwise, set the Qdrant instance address with:\n",
    "  - `uri=\"http://<host>:<port>\"`\n",
    "\n",
    "- Set the API KEY for Qdrant Cloud:\n",
    "  - `api_key=\"<qdrant-api-key>\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "client = QdrantClient(\n",
    "    url=\"https://486a0c22-f395-4a53-8ce3-26851a6e8b84.us-east-1-0.aws.cloud.qdrant.io:6333\", \n",
    "    api_key=os.environ[\"QDRANT_API_KEY\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data into Llama-Index\n",
    "Llama-Index has a simple way to load documents from a directory. We can define a function to get the metadata from a file name, and pass this function to the `SimpleDirectoryReader` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_metadata(file_name: str):\n",
    "    \"\"\"Get file metadata.\"\"\"\n",
    "    return {\"date\": Path(file_name).stem}\n",
    "\n",
    "\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_files=write_dir.ls(), file_metadata=get_file_metadata\n",
    ").load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then parse these documents into nodes and create our QdrantVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from llama_index import GPTVectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "\n",
    "# define service context (wrapper container around current classes)\n",
    "service_context = ServiceContext.from_defaults(chunk_size_limit=512)\n",
    "\n",
    "# use node parser in service context to parse into nodes\n",
    "nodes = service_context.node_parser.get_nodes_from_documents(documents)\n",
    "vector_store = QdrantVectorStore(client=client, collection_name=\"huffpostnews\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll create our `GPTVectorStoreIndex` from the documents. This operation might take some time as it's creating the index from the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.9 s, sys: 1.48 s, total: 15.4 s\n",
      "Wall time: 7min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = GPTVectorStoreIndex.from_documents(documents, storage_context=storage_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Recency Postprocessors\n",
    "\n",
    "Llama-Index allows us to add postprocessors to our query engine. These postprocessors can modify the results of our queries after they are returned from the index. Here, we'll add a recency postprocessor to our query engine. This postprocessor will prioritize recent documents in the results.\n",
    "\n",
    "We'll define two types of recency postprocessors - `FixedRecencyPostprocessor` and `EmbeddingRecencyPostprocessor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "recency_postprocessor = FixedRecencyPostprocessor(service_context=service_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incorporating Reranking using Cohere\n",
    "Cohere is a powerful AI model that can help us rerank our results. We'll define a `CohereRerank` postprocessor and add it to our query engine.\n",
    "\n",
    "First, install the cohere package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install cohere"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using it in our code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_index.indices.postprocessor.cohere_rerank import CohereRerank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Query Engines\n",
    "We'll define four query engines for this tutorial: \n",
    "1. Just the Vector Store i.e. Qdrant here\n",
    "1. A recency query engine\n",
    "1. A reranking query engine\n",
    "1. And a combined query engine.\n",
    "\n",
    "The recency query engine uses the `FixedRecencyPostprocessor`, the reranking query engine uses the `CohereRerank` postprocessor, and the combined query engine uses both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import Optional\n",
    "\n",
    "class Interaction(BaseModel):\n",
    "    question: str\n",
    "    answer: Optional[str] = None\n",
    "    response: Optional[object] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_query_engine = index.as_query_engine(\n",
    "    similarity_top_k=top_k,\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "recency_query_engine = index.as_query_engine(\n",
    "    similarity_top_k=top_k,\n",
    "    node_postprocessors=[recency_postprocessor],\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohere_rerank = CohereRerank(api_key=os.environ[\"COHERE_API_KEY\"], top_n=top_k)\n",
    "reranking_query_engine = index.as_query_engine(\n",
    "    similarity_top_k=top_k,\n",
    "    node_postprocessors=[cohere_rerank],\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k=top_k,\n",
    "    node_postprocessors=[cohere_rerank, recency_postprocessor],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying the Engine\n",
    "Finally, we can query our engine. Let's ask it \"Who is the current US President?\" and see the results from each query engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The current US President is Joe Biden.\n"
     ]
    }
   ],
   "source": [
    "question = \"Who is the current US President?\"\n",
    "response = index_query_engine.query(question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `response` object has a few interesting attributes which help us quickly debug and understand what happened in each of our steps: \n",
    "1. What source nodes (similar to Document Chunks in Langchain) were used to answer the question\n",
    "2. What `extra_info` does the index have which we can use? This could also be sent as a payload to Qdrant to filter on (via epoch time) -- but Llama Index does not\n",
    "\n",
    "Let's unpack that a bit, and we'll use what we learn from `response` to improve our understanding of the query engines and post processors themselves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list,\n",
       " 10,\n",
       " NodeWithScore(node=Node(text='date: 2021-06-13\\n\\nPresident Biden Meets With Queen Elizabeth At Windsor Castle under the category: WORLD NEWS\\n Biden, who is in the U.K. for the Group of Seven leaders summit, is the 13th American president\\xa0to meet the monarch.', doc_id='13340f61-1f07-493b-8475-cffac69797b5', embedding=None, doc_hash='83f86814916cf2880a3bc224ad557ee0e0cc90087aadf2f87784bc786f305129', extra_info={'date': '2021-06-13'}, node_info=None, relationships={<DocumentRelationship.SOURCE: '1'>: 'd3d9d1b6-10d5-450d-8f25-cca3f6c1ec00'}), score=0.8132833))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response.source_nodes), len(response.source_nodes), response.source_nodes[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `10` which is the topk parameter we set. This confirms that we got back the 10 documents most similar to the question (or more correct: 10 nearest neighbours to the question) and a confidence score. \n",
    "\n",
    "This can be made more human friendly perhaps?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Source (Doc id: 13340f61-1f07-493b-8475-cffac69797b5): date: 2021-06-13\n",
      "\n",
      "date: 2021-06-13\n",
      "\n",
      "President Biden Meets With Queen Elizabeth At Windsor Castle ...\n",
      "\n",
      "> Source (Doc id: 3fda0831-3839-4c11-86f4-b1935e17b709): date: 2016-11-13\n",
      "\n",
      "date: 2016-11-13\n",
      "\n",
      "Public Diplomacy in the Trump Era under the category: WORLDPO...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(response.get_formatted_sources()[:318])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check what is stored in the `extra_info` attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'13340f61-1f07-493b-8475-cffac69797b5': {'date': '2021-06-13'},\n",
       " '3fda0831-3839-4c11-86f4-b1935e17b709': {'date': '2016-11-13'},\n",
       " '0798b54e-18b0-48d1-a16b-c9566c2f62c3': {'date': '2014-08-02'},\n",
       " '5a1207be-9c1b-466e-b122-3912787a26e6': {'date': '2017-11-26'},\n",
       " '006bcdd0-c86e-4b4a-b72f-72bbe75b4870': {'date': '2015-02-22'},\n",
       " '288548e6-877e-43cc-97bf-45fe6a50cb16': {'date': '2017-05-14'},\n",
       " 'db90cb93-aeb5-4dba-a9a5-3129d88bdc1b': {'date': '2018-11-10'},\n",
       " '2935dbb7-da04-48e5-b84b-4023a18dc9c6': {'date': '2016-05-03'},\n",
       " '9db09aaa-0284-4f8a-976d-68d236ab3c8f': {'date': '2016-10-03'},\n",
       " 'df33aa4d-191e-40cd-badd-3b250f1d7903': {'date': '2016-06-05'}}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.extra_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has mostly a `date` key-value as a string against the `doc id`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_responses(question: str)->None:\n",
    "    responses = {}\n",
    "    print(\"*\"*len(question))\n",
    "    print(f\"Question: {question}\")\n",
    "    print(\"*\"*len(question))\n",
    "    response = index_query_engine.query(question)\n",
    "    responses[\"qdrant\"] = response\n",
    "    print(\"No Postprocessor, Qdrant:\", response)\n",
    "    response = recency_query_engine.query(question)\n",
    "    responses[\"recency\"] = response\n",
    "    print(\"Recency:\", response)\n",
    "    response = reranking_query_engine.query(question)\n",
    "    responses[\"reranking\"] = response\n",
    "    print(\"Reranking:\", response)\n",
    "    response = query_engine.query(question)\n",
    "    responses[\"both\"] = response\n",
    "    print(\"Both:\", response)\n",
    "    return responses\n",
    "\n",
    "responses = compare_responses(question)\n",
    "\"Correct answer is Joe Biden\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************\n",
      "Question: When did Mughals invade India?\n",
      "******************************\n",
      "No Postprocessor, Qdrant: \n",
      "The Mughals invaded India in 1526.\n",
      "Recency: \n",
      "The Mughal Empire began in 1526 when Babur, a Central Asian ruler, invaded India and established the Mughal dynasty.\n",
      "Reranking: \n",
      "The Mughals invaded India in the 16th century.\n",
      "Both: \n",
      "The Mughal Empire began in 1526 when Babur, a Central Asian ruler, invaded India and established the Mughal dynasty.\n",
      "**Correct answer** is Babur's victory in Battle of Panipat in April 1526\n"
     ]
    }
   ],
   "source": [
    "question = \"When did Mughals invade India?\"\n",
    "responses = compare_responses(question)\n",
    "\n",
    "print(\"**Correct answer** is Babur's victory in Battle of Panipat in April 1526\")\n",
    "# responses[\"both\"].source_nodes, responses[\"qdrant\"].source_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************\n",
      "Question: Who won the latest Superbowl?\n",
      "*****************************\n",
      "No Postprocessor, Qdrant: \n",
      "The latest Super Bowl was Super Bowl LVI, which took place on February 13, 2022. The winner of Super Bowl LVI has not yet been determined.\n",
      "Recency: \n",
      "The Los Angeles Rams won the latest Super Bowl.\n",
      "Reranking: \n",
      "The latest Super Bowl was Super Bowl LVI, which was held on February 13, 2022. The Los Angeles Rams defeated the Cincinnati Bengals by a score of 31-23.\n",
      "Both: \n",
      "The Los Angeles Rams won the latest Super Bowl.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Correct answer is Kansas Chiefs in May 2023, but the data cutoff is September 2022 for which Los Angeles Rams is the correct answer'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"Who won the latest Superbowl?\"\n",
    "responses = compare_responses(question)\n",
    "\"Correct answer is Kansas Chiefs in May 2023, but the data cutoff is September 2022 for which Los Angeles Rams is the correct answer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************\n",
      "Question: When did Kargil war happen? Why is that significant?\n",
      "****************************************************\n",
      "No Postprocessor, Qdrant: \n",
      "The Kargil War happened in 1999. It was significant because it was the first major armed conflict between India and Pakistan since the 1971 Indo-Pakistani War, and it was fought in the Kargil district of Kashmir and elsewhere along the Line of Control (LOC). The conflict is also noted for being one of the first instances of high-altitude warfare in the world, as the two sides fought in the Himalayan mountains. The war resulted in a strategic victory for India, as it successfully managed to push back the Pakistani forces from the Kargil district.\n",
      "Recency: \n",
      "The Kargil War happened in 1999. It was significant because it was the first major armed conflict between India and Pakistan since the 1971 Indo-Pakistani War. The conflict resulted in a strategic victory for India, and it is seen as a major turning point in the relationship between the two countries.\n",
      "Reranking: \n",
      "The Kargil War happened in 1999. It was significant because it was the first major armed conflict between India and Pakistan since the 1971 Indo-Pakistani War, and it was the first time since the Partition of India in 1947 that the two nuclear-armed powers had engaged in direct military conflict. The conflict resulted in the loss of hundreds of lives and caused a major diplomatic crisis between the two countries.\n",
      "Both: \n",
      "The Kargil War happened in 1999. It was significant because it was the first major armed conflict between India and Pakistan since the 1971 Indo-Pakistani War. The conflict resulted in a strategic victory for India, and it is seen as a major turning point in the relationship between the two countries.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1999, 2 nuclear countries went to war for the first time'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"When did Kargil war happen? Why is that significant?\"\n",
    "responses = compare_responses(question)\n",
    "\"1999, 2 nuclear countries went to war for the first time\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************************************\n",
      "Question: Who is the Indian Prime Minister in January 2014?\n",
      "*************************************************\n",
      "No Postprocessor, Qdrant: \n",
      "The Indian Prime Minister in January 2014 was Manmohan Singh.\n",
      "Recency: \n",
      "The Indian Prime Minister in January 2014 was Narendra Modi.\n",
      "Reranking: \n",
      "The Indian Prime Minister in January 2014 was Manmohan Singh.\n",
      "Both: \n",
      "The Indian Prime Minister in January 2014 was Narendra Modi.\n"
     ]
    }
   ],
   "source": [
    "question = \"Who is the Indian Prime Minister in January 2014?\"\n",
    "responses = compare_responses(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this question: \"Who is the Indian Prime Minister in January 2014?\", the Qdrant and reranking engines correctly respond with \"Manmohan Singh\", while the recency and combined engines incorrectly respond with \"Narendra Modi\". \n",
    "\n",
    "This demonstrates how the use of recency postprocessing can lead to incorrect results if the recency information contradicts the correct answer. Let's do quick check to see if we can understand why this happened: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
