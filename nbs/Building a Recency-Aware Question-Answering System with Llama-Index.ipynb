{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Building a Recency-Aware Question-Answering System with Llama-Index\n",
    "\n",
    "\n",
    "In this tutorial, we will learn how to build a question-answering system that is aware of the recency of information. This is especially important for questions whose answers may change over time, such as \"Who is the current US President?\".\n",
    "\n",
    "We will be using the llama_index package, a powerful tool for building large-scale information retrieval systems.\n",
    "It is designed to work with language models like OpenAI's GPT-3. It also supports the use of postprocessors, which can modify the results of a query after they have been returned from the index. We demonstrate the impact of two interesting postprocessors.\n",
    "\n",
    "Qdrant is a high-performance vector database designed for storing and searching large-scale high-dimensional vectors. In the context of this notebook, we use Qdrant as our vector storage system.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before you begin, API Keys:\n",
    "\n",
    "Dependency: \n",
    "1. OpenAI keys for LLM\n",
    "2. Cohere keys for Reranking\n",
    "\n",
    "If you'd like to use Qdrant Cloud, that needs: \n",
    "1. URL\n",
    "2. API Keys\n",
    "\n",
    "## Installing necessary packages\n",
    "Before we start, make sure that you have the `llama_index` package installed. You can install it using pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expected ':' (413961974.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[15], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    def check_environment_keys()\u001b[0m\n\u001b[0m                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m expected ':'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def check_environment_keys():\n",
    "    \"\"\"\n",
    "    Utility Function that you've the NECESSARY Keys\n",
    "    \"\"\"\n",
    "    if os.environ.get(\"OPENAI_API_KEY\") is None:\n",
    "        raise ValueError(\"OPENAI_API_KEY cannot be None.\")\n",
    "    if os.environ.get(\"COHERE_API_KEY\") is None:\n",
    "        raise ValueError(\"COHERE_API_KEY cannot be None.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-index\n",
      "  Using cached llama_index-0.6.8-py3-none-any.whl (389 kB)\n",
      "Collecting dataclasses-json\n",
      "  Using cached dataclasses_json-0.5.7-py3-none-any.whl (25 kB)\n",
      "Collecting langchain>=0.0.154\n",
      "  Downloading langchain-0.0.173-py3-none-any.whl (858 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m858.2/858.2 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting numpy\n",
      "  Using cached numpy-1.24.3-cp311-cp311-macosx_11_0_arm64.whl (13.8 MB)\n",
      "Collecting tenacity<9.0.0,>=8.2.0\n",
      "  Using cached tenacity-8.2.2-py3-none-any.whl (24 kB)\n",
      "Collecting openai>=0.26.4\n",
      "  Using cached openai-0.27.6-py3-none-any.whl (71 kB)\n",
      "Collecting pandas\n",
      "  Using cached pandas-2.0.1-cp311-cp311-macosx_11_0_arm64.whl (10.7 MB)\n",
      "Requirement already satisfied: requests<2.30.0 in /opt/homebrew/Caskroom/miniconda/base/envs/qdrant/lib/python3.11/site-packages (from llama-index) (2.29.0)\n",
      "Collecting tiktoken\n",
      "  Using cached tiktoken-0.4.0-cp311-cp311-macosx_11_0_arm64.whl (761 kB)\n",
      "Collecting PyYAML>=5.4.1\n",
      "  Using cached PyYAML-6.0-cp311-cp311-macosx_11_0_arm64.whl (167 kB)\n",
      "Collecting SQLAlchemy<3,>=1.4\n",
      "  Downloading SQLAlchemy-2.0.13-cp311-cp311-macosx_11_0_arm64.whl (2.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hCollecting aiohttp<4.0.0,>=3.8.3\n",
      "  Using cached aiohttp-3.8.4-cp311-cp311-macosx_11_0_arm64.whl (332 kB)\n",
      "Collecting numexpr<3.0.0,>=2.8.4\n",
      "  Using cached numexpr-2.8.4-cp311-cp311-macosx_11_0_arm64.whl (89 kB)\n",
      "Collecting openapi-schema-pydantic<2.0,>=1.2\n",
      "  Using cached openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
      "Collecting pydantic<2,>=1\n",
      "  Using cached pydantic-1.10.7-cp311-cp311-macosx_11_0_arm64.whl (2.5 MB)\n",
      "Collecting marshmallow<4.0.0,>=3.3.0\n",
      "  Using cached marshmallow-3.19.0-py3-none-any.whl (49 kB)\n",
      "Collecting marshmallow-enum<2.0.0,>=1.5.1\n",
      "  Using cached marshmallow_enum-1.5.1-py2.py3-none-any.whl (4.2 kB)\n",
      "Collecting typing-inspect>=0.4.0\n",
      "  Using cached typing_inspect-0.8.0-py3-none-any.whl (8.7 kB)\n",
      "Collecting tqdm\n",
      "  Using cached tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/Caskroom/miniconda/base/envs/qdrant/lib/python3.11/site-packages (from requests<2.30.0->llama-index) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/Caskroom/miniconda/base/envs/qdrant/lib/python3.11/site-packages (from requests<2.30.0->llama-index) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/homebrew/Caskroom/miniconda/base/envs/qdrant/lib/python3.11/site-packages (from requests<2.30.0->llama-index) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/Caskroom/miniconda/base/envs/qdrant/lib/python3.11/site-packages (from requests<2.30.0->llama-index) (2023.5.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/homebrew/Caskroom/miniconda/base/envs/qdrant/lib/python3.11/site-packages (from pandas->llama-index) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/Caskroom/miniconda/base/envs/qdrant/lib/python3.11/site-packages (from pandas->llama-index) (2022.7)\n",
      "Collecting tzdata>=2022.1\n",
      "  Using cached tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "Collecting regex>=2022.1.18\n",
      "  Using cached regex-2023.5.5-cp311-cp311-macosx_11_0_arm64.whl (288 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/homebrew/Caskroom/miniconda/base/envs/qdrant/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.154->llama-index) (22.1.0)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Using cached multidict-6.0.4-cp311-cp311-macosx_11_0_arm64.whl (29 kB)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Using cached async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Using cached yarl-1.9.2-cp311-cp311-macosx_11_0_arm64.whl (61 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Using cached frozenlist-1.3.3-cp311-cp311-macosx_11_0_arm64.whl (34 kB)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: packaging>=17.0 in /opt/homebrew/Caskroom/miniconda/base/envs/qdrant/lib/python3.11/site-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json->llama-index) (23.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/homebrew/Caskroom/miniconda/base/envs/qdrant/lib/python3.11/site-packages (from pydantic<2,>=1->langchain>=0.0.154->llama-index) (4.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/Caskroom/miniconda/base/envs/qdrant/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->llama-index) (1.16.0)\n",
      "Collecting mypy-extensions>=0.3.0\n",
      "  Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: tzdata, tqdm, tenacity, SQLAlchemy, regex, PyYAML, pydantic, numpy, mypy-extensions, multidict, marshmallow, frozenlist, async-timeout, yarl, typing-inspect, tiktoken, pandas, openapi-schema-pydantic, numexpr, marshmallow-enum, aiosignal, dataclasses-json, aiohttp, openai, langchain, llama-index\n",
      "Successfully installed PyYAML-6.0 SQLAlchemy-2.0.13 aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 dataclasses-json-0.5.7 frozenlist-1.3.3 langchain-0.0.173 llama-index-0.6.8 marshmallow-3.19.0 marshmallow-enum-1.5.1 multidict-6.0.4 mypy-extensions-1.0.0 numexpr-2.8.4 numpy-1.24.3 openai-0.27.6 openapi-schema-pydantic-1.2.4 pandas-2.0.1 pydantic-1.10.7 regex-2023.5.5 tenacity-8.2.2 tiktoken-0.4.0 tqdm-4.65.0 typing-inspect-0.8.0 tzdata-2023.3 yarl-1.9.2\n"
     ]
    }
   ],
   "source": [
    "# !pip install llama-index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will also need the `qdrant_client` package for this tutorial. You can install it via pip as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting qdrant-client\n",
      "  Downloading qdrant_client-1.1.7-py3-none-any.whl (127 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.8/127.8 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting grpcio>=1.41.0\n",
      "  Downloading grpcio-1.54.2-cp311-cp311-macosx_10_10_universal2.whl (8.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting grpcio-tools>=1.41.0\n",
      "  Downloading grpcio_tools-1.54.2-cp311-cp311-macosx_10_10_universal2.whl (3.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting httpx[http2]>=0.14.0\n",
      "  Using cached httpx-0.24.0-py3-none-any.whl (75 kB)\n",
      "Requirement already satisfied: numpy>=1.21 in /opt/homebrew/Caskroom/miniconda/base/envs/qdrant/lib/python3.11/site-packages (from qdrant-client) (1.24.3)\n",
      "Collecting portalocker<3.0.0,>=2.7.0\n",
      "  Using cached portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: pydantic<2.0,>=1.8 in /opt/homebrew/Caskroom/miniconda/base/envs/qdrant/lib/python3.11/site-packages (from qdrant-client) (1.10.7)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.0.0 in /opt/homebrew/Caskroom/miniconda/base/envs/qdrant/lib/python3.11/site-packages (from qdrant-client) (4.5.0)\n",
      "Requirement already satisfied: urllib3<2.0.0,>=1.26.14 in /opt/homebrew/Caskroom/miniconda/base/envs/qdrant/lib/python3.11/site-packages (from qdrant-client) (1.26.15)\n",
      "Collecting protobuf<5.0dev,>=4.21.6\n",
      "  Downloading protobuf-4.23.1-cp37-abi3-macosx_10_9_universal2.whl (400 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.3/400.3 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /opt/homebrew/Caskroom/miniconda/base/envs/qdrant/lib/python3.11/site-packages (from grpcio-tools>=1.41.0->qdrant-client) (66.0.0)\n",
      "Requirement already satisfied: certifi in /opt/homebrew/Caskroom/miniconda/base/envs/qdrant/lib/python3.11/site-packages (from httpx[http2]>=0.14.0->qdrant-client) (2023.5.7)\n",
      "Collecting httpcore<0.18.0,>=0.15.0\n",
      "  Downloading httpcore-0.17.1-py3-none-any.whl (70 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.9/70.9 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: idna in /opt/homebrew/Caskroom/miniconda/base/envs/qdrant/lib/python3.11/site-packages (from httpx[http2]>=0.14.0->qdrant-client) (3.4)\n",
      "Requirement already satisfied: sniffio in /opt/homebrew/Caskroom/miniconda/base/envs/qdrant/lib/python3.11/site-packages (from httpx[http2]>=0.14.0->qdrant-client) (1.2.0)\n",
      "Collecting h2<5,>=3\n",
      "  Using cached h2-4.1.0-py3-none-any.whl (57 kB)\n",
      "Collecting hyperframe<7,>=6.0\n",
      "  Using cached hyperframe-6.0.1-py3-none-any.whl (12 kB)\n",
      "Collecting hpack<5,>=4.0\n",
      "  Using cached hpack-4.0.0-py3-none-any.whl (32 kB)\n",
      "Collecting h11<0.15,>=0.13\n",
      "  Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /opt/homebrew/Caskroom/miniconda/base/envs/qdrant/lib/python3.11/site-packages (from httpcore<0.18.0,>=0.15.0->httpx[http2]>=0.14.0->qdrant-client) (3.5.0)\n",
      "Installing collected packages: protobuf, portalocker, hyperframe, hpack, h11, grpcio, httpcore, h2, grpcio-tools, httpx, qdrant-client\n",
      "Successfully installed grpcio-1.54.2 grpcio-tools-1.54.2 h11-0.14.0 h2-4.1.0 hpack-4.0.0 httpcore-0.17.1 httpx-0.24.0 hyperframe-6.0.1 portalocker-2.7.0 protobuf-4.23.1 qdrant-client-1.1.7\n"
     ]
    }
   ],
   "source": [
    "!pip install -U qdrant-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The rich extension is already loaded. To reload it, use:\n",
      "  %reload_ext rich\n"
     ]
    }
   ],
   "source": [
    "# Optionally: Rich to make error messages, stack traces easier to read\n",
    "# !pip install 'rich[jupyter]'\n",
    "%load_ext rich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import qdrant_client\n",
    "from datetime import datetime\n",
    "\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from pathlib import Path\n",
    "from llama_index import GPTVectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
    "from llama_index.indices.postprocessor import FixedRecencyPostprocessor\n",
    "\n",
    "# load documents\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "\n",
    "Path.ls = lambda x: list(x.iterdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Documents\n",
    "\n",
    "First, let's load our documents. In this example, we will use the News Category Dataset v3. This dataset contains news articles with various fields like `headline`, `category`, `short_description`, `link`, `authors`, and date. We'll load the data, and reformat it to suit our needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"../data/News_Category_Dataset_v3.json\", \"r\") as f:\n",
    "    data = [json.loads(k) for k in f.readlines()]\n",
    "    links = [k.pop(\"link\") for k in data]\n",
    "    authors = [k.pop(\"authors\") for k in data]\n",
    "    data = [\n",
    "        {\n",
    "            \"text\": f\"{k['headline']} under the category: {k['category']}\\n {k['short_description']}\",\n",
    "            \"date\": k[\"date\"],\n",
    "        }\n",
    "        for k in data\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(datetime.datetime(2012, 1, 28, 0, 0), datetime.datetime(2022, 9, 23, 0, 0))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "dates = [datetime.datetime.fromisoformat(element[\"date\"]) for element in data]\n",
    "min(dates), max(dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll write these documents to text files in a directory. Each document will be written to a text file named by its date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "write_dir = Path(\"../data/dump\").resolve()\n",
    "write_dir.mkdir(exist_ok=True, parents=True)\n",
    "for element in data:\n",
    "    file_path = write_dir / f\"{element['date']}.txt\"\n",
    "    with file_path.open(\"w\") as f:\n",
    "        #         print(element[\"text\"])\n",
    "        f.write(element[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What do we want to build?\n",
    "\n",
    "This diagram shows that:\n",
    "\n",
    "1. The user issues a query to the query engine.\n",
    "2. The query engine, which has been configured with certain postprocessors, performs a search on the vector store based on the query.\n",
    "3. The query engine then postprocesses the results.\n",
    "4. The postprocessed results are then returned to the user.\n",
    "\n",
    "Each arrow represents the direction of data flow. The \"Query Engine\" box encapsulates the postprocessing step to indicate that it's a part of the query engine's function. This diagram is meant to provide a high-level understanding of the process and does not include all the details involved.\n",
    "\n",
    "\n",
    "We're using Llama Index to make the Query Engine and Qdrant for our Vector Store\n",
    "\n",
    "![](../images/SetupFocus.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Qdrant Client\n",
    "We'll be using Qdrant as our vector storage system. Qdrant is a high-performance vector database designed for storing and searching large-scale high-dimensional vectors.\n",
    "\n",
    "### Local Qdrant Server/Docker + Cloud Instructions\n",
    "- If you're running a local Qdrant instance with Docker, use `uri`:\n",
    "  - `uri=\"http://<host>:<port>\"`\n",
    "  \n",
    "Here I'll be using the cloud, so I am using the url set to my cloud instance\n",
    "\n",
    "- Set the API KEY for Qdrant Cloud:\n",
    "  - `api_key=\"<qdrant-api-key>\"`\n",
    "  - `url`\n",
    "\n",
    "### Memory\n",
    "\n",
    "- You can use `:memory:` mode for fast and lightweight experiments. It does not require Qdrant to be deployed anywhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "client = QdrantClient(\":memory:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data into Llama-Index\n",
    "Llama-Index has a simple way to load documents from a directory. We can define a function to get the metadata from a file name, and pass this function to the `SimpleDirectoryReader` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_metadata(file_name: str):\n",
    "    \"\"\"Get file metadata.\"\"\"\n",
    "    return {\"date\": Path(file_name).stem}\n",
    "\n",
    "\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_files=write_dir.ls(), file_metadata=get_file_metadata\n",
    ").load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `date` key is *necessary* for the Recency Postprocessor that we'll use later.\n",
    "\n",
    "We then parse these documents into nodes and create our QdrantVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for OpenAI\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass  `openai_api_key` as a named parameter. (type=value_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvector_stores\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mqdrant\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m QdrantVectorStore\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# define service context (wrapper container around current classes)\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m service_context \u001b[38;5;241m=\u001b[39m ServiceContext\u001b[38;5;241m.\u001b[39mfrom_defaults(chunk_size_limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m)\n\u001b[1;32m      7\u001b[0m vector_store \u001b[38;5;241m=\u001b[39m QdrantVectorStore(client\u001b[38;5;241m=\u001b[39mclient, collection_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuffpostnews\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/qdrant/lib/python3.11/site-packages/llama_index/indices/service_context.py:83\u001b[0m, in \u001b[0;36mServiceContext.from_defaults\u001b[0;34m(cls, llm_predictor, prompt_helper, embed_model, node_parser, llama_logger, callback_manager, chunk_size_limit)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Create a ServiceContext from defaults.\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03mIf an argument is specified, then use the argument value provided for that\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03mparameter. If an argument is not specified, then use the default value.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     80\u001b[0m \n\u001b[1;32m     81\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     82\u001b[0m callback_manager \u001b[38;5;241m=\u001b[39m callback_manager \u001b[38;5;129;01mor\u001b[39;00m CallbackManager([])\n\u001b[0;32m---> 83\u001b[0m llm_predictor \u001b[38;5;241m=\u001b[39m llm_predictor \u001b[38;5;129;01mor\u001b[39;00m LLMPredictor()\n\u001b[1;32m     84\u001b[0m llm_predictor\u001b[38;5;241m.\u001b[39mcallback_manager \u001b[38;5;241m=\u001b[39m callback_manager\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# NOTE: the embed_model isn't used in all indices\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/qdrant/lib/python3.11/site-packages/llama_index/llm_predictor/base.py:174\u001b[0m, in \u001b[0;36mLLMPredictor.__init__\u001b[0;34m(self, llm, retry_on_throttling, cache, callback_manager)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    168\u001b[0m     llm: Optional[BaseLanguageModel] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    171\u001b[0m     callback_manager: Optional[CallbackManager] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    172\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    173\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Initialize params.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 174\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_llm \u001b[38;5;241m=\u001b[39m llm \u001b[38;5;129;01mor\u001b[39;00m OpenAI(temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-davinci-003\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    176\u001b[0m         langchain\u001b[38;5;241m.\u001b[39mllm_cache \u001b[38;5;241m=\u001b[39m cache\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/qdrant/lib/python3.11/site-packages/pydantic/main.py:341\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for OpenAI\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass  `openai_api_key` as a named parameter. (type=value_error)"
     ]
    }
   ],
   "source": [
    "from llama_index import GPTVectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "\n",
    "# define service context (wrapper container around current classes)\n",
    "service_context = ServiceContext.from_defaults(chunk_size_limit=512)\n",
    "\n",
    "vector_store = QdrantVectorStore(client=client, collection_name=\"huffpostnews\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3890,\n",
       " Node(text=\"'Hero Of The Year' Reunites Soldiers With Stray Dogs They Befriended Overseas under the category: IMPACT\\n \", doc_id='fe35e06e-1222-4d67-a8e3-ce525b54311c', embedding=None, doc_hash='5c8e684e7fe149aafbdd18b25ee59ce336fdef3d23a417e0f25b08cdd8a19a7a', extra_info={'date': '2014-12-08'}, node_info={'start': 0, 'end': 106}, relationships={<DocumentRelationship.SOURCE: '1'>: 'c7a5f388-2385-471d-90cb-28e342943b79'}))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use node parser in service context to parse into nodes\n",
    "nodes = service_context.node_parser.get_nodes_from_documents(documents)\n",
    "\n",
    "len(nodes), nodes[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll create our `GPTVectorStoreIndex` from the documents. This operation might take some time as it's creating the index from the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.9 s, sys: 1.48 s, total: 15.4 s\n",
      "Wall time: 7min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = GPTVectorStoreIndex.from_documents(documents, storage_context=storage_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Recency Postprocessors\n",
    "\n",
    "Llama-Index allows us to add postprocessors to our query engine. These postprocessors can modify the results of our queries after they are returned from the index. Here, we'll add a recency postprocessor to our query engine. This postprocessor will prioritize recent documents in the results.\n",
    "\n",
    "We'll define a single type of recency postprocessors - `FixedRecencyPostprocessor`.\n",
    "\n",
    "Llama Index also has a `EmbeddingRecencyPostprocessor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "recency_postprocessor = FixedRecencyPostprocessor(service_context=service_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incorporating Reranking using Cohere\n",
    "Cohere is a powerful AI model that can help us rerank our results. We'll define a `CohereRerank` postprocessor and add it to our query engine. This can improve the search results since it's working on a smaller dataset. \n",
    "\n",
    "First, install the cohere package. You will also need a [Cohere API Key](https://docs.cohere.com/reference/key) and add to your environment. One of the ways to do can do so is this:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"COHERE_API_KEY\"] = \"<api-key>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using it in our code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_index.indices.postprocessor.cohere_rerank import CohereRerank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Query Engines\n",
    "We'll define four query engines for this tutorial: \n",
    "1. Just the Vector Store i.e. Qdrant here\n",
    "1. A recency query engine\n",
    "1. A reranking query engine\n",
    "1. And a combined query engine.\n",
    "\n",
    "The recency query engine uses the `FixedRecencyPostprocessor`, the reranking query engine uses the `CohereRerank` postprocessor, and the combined query engine uses both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 10  # set one, reuse from now on, ensures consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_query_engine = index.as_query_engine(\n",
    "    similarity_top_k=top_k,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "recency_query_engine = index.as_query_engine(\n",
    "    similarity_top_k=top_k,\n",
    "    node_postprocessors=[recency_postprocessor],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohere_rerank = CohereRerank(api_key=os.environ[\"COHERE_API_KEY\"], top_n=top_k)\n",
    "reranking_query_engine = index.as_query_engine(\n",
    "    similarity_top_k=top_k,\n",
    "    node_postprocessors=[cohere_rerank],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k=top_k,\n",
    "    node_postprocessors=[cohere_rerank, recency_postprocessor],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying the Engine\n",
    "Finally, we can query our engine. Let's ask it \"Who is the current US President?\" and see the results from each query engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The current US President is Joe Biden.\n"
     ]
    }
   ],
   "source": [
    "question = \"Who is the current US President?\"\n",
    "response = index_query_engine.query(question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `response` object has a few interesting attributes which help us quickly debug and understand what happened in each of our steps: \n",
    "1. What source nodes (similar to Document Chunks in Langchain) were used to answer the question\n",
    "2. What `extra_info` does the index have which we can use? This could also be sent as a payload to Qdrant to filter on (via epoch time) -- but Llama Index does not\n",
    "\n",
    "Let's unpack that a bit, and we'll use what we learn from `response` to improve our understanding of the query engines and post processors themselves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list,\n",
       " 10,\n",
       " NodeWithScore(node=Node(text='date: 2021-06-13\\n\\nPresident Biden Meets With Queen Elizabeth At Windsor Castle under the category: WORLD NEWS\\n Biden, who is in the U.K. for the Group of Seven leaders summit, is the 13th American president\\xa0to meet the monarch.', doc_id='13340f61-1f07-493b-8475-cffac69797b5', embedding=None, doc_hash='83f86814916cf2880a3bc224ad557ee0e0cc90087aadf2f87784bc786f305129', extra_info={'date': '2021-06-13'}, node_info=None, relationships={<DocumentRelationship.SOURCE: '1'>: 'd3d9d1b6-10d5-450d-8f25-cca3f6c1ec00'}), score=0.8132833))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response.source_nodes), len(response.source_nodes), response.source_nodes[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `10` which is the topk parameter we set. This confirms that we got back the 10 documents most similar to the question (or more correct: 10 nearest neighbours to the question) and a confidence score. \n",
    "\n",
    "This can be made more human friendly perhaps?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Source (Doc id: 13340f61-1f07-493b-8475-cffac69797b5): date: 2021-06-13\n",
      "\n",
      "date: 2021-06-13\n",
      "\n",
      "President Biden Meets With Queen Elizabeth At Windsor Castle ...\n",
      "\n",
      "> Source (Doc id: 3fda0831-3839-4c11-86f4-b1935e17b709): date: 2016-11-13\n",
      "\n",
      "date: 2016-11-13\n",
      "\n",
      "Public Diplomacy in the Trump Era under the category: WORLDPO...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(response.get_formatted_sources()[:318])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check what is stored in the `extra_info` attribute. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'13340f61-1f07-493b-8475-cffac69797b5': {'date': '2021-06-13'},\n",
       " '3fda0831-3839-4c11-86f4-b1935e17b709': {'date': '2016-11-13'},\n",
       " '0798b54e-18b0-48d1-a16b-c9566c2f62c3': {'date': '2014-08-02'},\n",
       " '5a1207be-9c1b-466e-b122-3912787a26e6': {'date': '2017-11-26'},\n",
       " '006bcdd0-c86e-4b4a-b72f-72bbe75b4870': {'date': '2015-02-22'},\n",
       " '288548e6-877e-43cc-97bf-45fe6a50cb16': {'date': '2017-05-14'},\n",
       " 'db90cb93-aeb5-4dba-a9a5-3129d88bdc1b': {'date': '2018-11-10'},\n",
       " '2935dbb7-da04-48e5-b84b-4023a18dc9c6': {'date': '2016-05-03'},\n",
       " '9db09aaa-0284-4f8a-976d-68d236ab3c8f': {'date': '2016-10-03'},\n",
       " 'df33aa4d-191e-40cd-badd-3b250f1d7903': {'date': '2016-06-05'}}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.extra_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has a `date` key-value as a string against the `doc id`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's setup some tools to have a question, answer and the responses from the index engine in the same object - this will come handy in a bit for explaining a wrong answer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAInfo:\n",
    "    \"\"\"This class is used to store the question, correct answer and responses from different query engines.\"\"\"\n",
    "\n",
    "    def __init__(self, question: str, correct_answer: str):\n",
    "        self.question = question\n",
    "        self.correct_answer = correct_answer\n",
    "        self.responses = {}\n",
    "\n",
    "    def add_response(self, engine: str, response: str):\n",
    "        # This method is used to add the response of a query engine to the responses dictionary.\n",
    "        self.responses[engine] = response\n",
    "\n",
    "\n",
    "def compare_responses(qa_info: QAInfo, query_engines: dict):\n",
    "    \"\"\"This function takes in a QAInfo object and a dictionary of query engines, and runs the question through each query engine.\n",
    "    The responses from each engine are added to the QAInfo object.\"\"\"\n",
    "    print(\"*\" * len(qa_info.question))\n",
    "    print(f\"Question: {qa_info.question}\")\n",
    "    print(\"*\" * len(qa_info.question))\n",
    "\n",
    "    for engine_name, engine in query_engines.items():\n",
    "        response = engine.query(qa_info.question)\n",
    "        qa_info.add_response(engine_name, response)\n",
    "        print(f\"{engine_name}:\", response)\n",
    "\n",
    "    return qa_info\n",
    "\n",
    "\n",
    "query_engines = {\n",
    "    \"qdrant\": index_query_engine,\n",
    "    \"recency\": recency_query_engine,\n",
    "    \"reranking\": reranking_query_engine,\n",
    "    \"both\": query_engine,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************\n",
      "Question: Who is the current US President?\n",
      "********************************\n",
      "qdrant: \n",
      "The current US President is Joe Biden.\n",
      "recency: \n",
      "The current US President is Joe Biden.\n",
      "reranking: \n",
      "The current US President is Joe Biden.\n",
      "both: \n",
      "The current US President is Joe Biden.\n",
      "Correct answer is Joe Biden\n"
     ]
    }
   ],
   "source": [
    "question = \"Who is the current US President?\"\n",
    "correct_answer = \"Joe Biden\"  # This would normally be determined programmatically.\n",
    "qa_info = QAInfo(question, correct_answer)\n",
    "qa_info = compare_responses(qa_info, query_engines)\n",
    "print(\"Correct answer is\", qa_info.correct_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************\n",
      "Question: When did Mughals invade India?\n",
      "******************************\n",
      "No Postprocessor, Qdrant: \n",
      "The Mughals invaded India in 1526.\n",
      "Recency: \n",
      "The Mughal Empire began in 1526 when Babur, a Central Asian ruler, invaded India and established the Mughal dynasty.\n",
      "Reranking: \n",
      "The Mughals invaded India in the 16th century.\n",
      "Both: \n",
      "The Mughal Empire began in 1526 when Babur, a Central Asian ruler, invaded India and established the Mughal dynasty.\n",
      "**Correct answer** is Babur's victory in Battle of Panipat in April 1526\n"
     ]
    }
   ],
   "source": [
    "question = \"When did Mughals invade India?\"\n",
    "responses = compare_responses(question)\n",
    "\n",
    "print(\"**Correct answer** is Babur's victory in Battle of Panipat in April 1526\")\n",
    "# responses[\"both\"].source_nodes, responses[\"qdrant\"].source_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************\n",
      "Question: Who won the latest Superbowl?\n",
      "*****************************\n",
      "No Postprocessor, Qdrant: \n",
      "The latest Super Bowl was Super Bowl LVI, which took place on February 13, 2022. The winner of Super Bowl LVI has not yet been determined.\n",
      "Recency: \n",
      "The Los Angeles Rams won the latest Super Bowl.\n",
      "Reranking: \n",
      "The latest Super Bowl was Super Bowl LVI, which was held on February 13, 2022. The Los Angeles Rams defeated the Cincinnati Bengals by a score of 31-23.\n",
      "Both: \n",
      "The Los Angeles Rams won the latest Super Bowl.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Correct answer is Kansas Chiefs in May 2023, but the data cutoff is September 2022 for which Los Angeles Rams is the correct answer'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"Who won the latest Superbowl?\"\n",
    "responses = compare_responses(question)\n",
    "\"Correct answer is Kansas Chiefs in May 2023, but the data cutoff is September 2022 for which Los Angeles Rams is the correct answer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************\n",
      "Question: When did Kargil war happen? Why is that significant?\n",
      "****************************************************\n",
      "No Postprocessor, Qdrant: \n",
      "The Kargil War happened in 1999. It was significant because it was the first major armed conflict between India and Pakistan since the 1971 Indo-Pakistani War, and it was fought in the Kargil district of Kashmir and elsewhere along the Line of Control (LOC). The conflict is also noted for being one of the first instances of high-altitude warfare in the world, as the two sides fought in the Himalayan mountains. The war resulted in a strategic victory for India, as it successfully managed to push back the Pakistani forces from the Kargil district.\n",
      "Recency: \n",
      "The Kargil War happened in 1999. It was significant because it was the first major armed conflict between India and Pakistan since the 1971 Indo-Pakistani War. The conflict resulted in a strategic victory for India, and it is seen as a major turning point in the relationship between the two countries.\n",
      "Reranking: \n",
      "The Kargil War happened in 1999. It was significant because it was the first major armed conflict between India and Pakistan since the 1971 Indo-Pakistani War, and it was the first time since the Partition of India in 1947 that the two nuclear-armed powers had engaged in direct military conflict. The conflict resulted in the loss of hundreds of lives and caused a major diplomatic crisis between the two countries.\n",
      "Both: \n",
      "The Kargil War happened in 1999. It was significant because it was the first major armed conflict between India and Pakistan since the 1971 Indo-Pakistani War. The conflict resulted in a strategic victory for India, and it is seen as a major turning point in the relationship between the two countries.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1999, 2 nuclear countries went to war for the first time'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"When did Kargil war happen? Why is that significant?\"\n",
    "responses = compare_responses(question)\n",
    "\"1999, 2 nuclear countries went to war for the first time\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************************************\n",
      "Question: Who is the Indian Prime Minister in January 2014?\n",
      "*************************************************\n",
      "No Postprocessor, Qdrant: \n",
      "The Indian Prime Minister in January 2014 was Manmohan Singh.\n",
      "Recency: \n",
      "The Indian Prime Minister in January 2014 was Narendra Modi.\n",
      "Reranking: \n",
      "The Indian Prime Minister in January 2014 was Manmohan Singh.\n",
      "Both: \n",
      "The Indian Prime Minister in January 2014 was Narendra Modi.\n"
     ]
    }
   ],
   "source": [
    "question = \"Who is the Indian Prime Minister in January 2014?\"\n",
    "responses = compare_responses(question)\n",
    "\"The correct answer is Manmohan Singh\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this question: \"Who is the Indian Prime Minister in January 2014?\", the Qdrant and reranking engines correctly respond with \"Manmohan Singh\", while the recency and combined engines incorrectly respond with \"Narendra Modi\". \n",
    "\n",
    "This demonstrates how the use of recency postprocessing can lead to incorrect results if the recency information contradicts the correct answer. Let's do quick check to see if we can understand why this happened: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********************************\n",
      "Question: Who was the US President in 2010?\n",
      "*********************************\n",
      "qdrant: \n",
      "The US President in 2010 was Barack Obama.\n",
      "recency: \n",
      "The US President in 2010 was Barack Obama.\n",
      "reranking: \n",
      "The US President in 2010 was Barack Obama.\n",
      "both: \n",
      "The US President in 2010 was Barack Obama.\n",
      "Correct answer is Obama\n"
     ]
    }
   ],
   "source": [
    "question = \"Who was the US President in 2010?\"\n",
    "correct_answer = \"Obama\"  # This would normally be determined programmatically.\n",
    "qa_info = QAInfo(question, correct_answer)\n",
    "qa_info = compare_responses(qa_info, query_engines)\n",
    "print(\"Correct answer is\", qa_info.correct_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k=top_k,\n",
    "    node_postprocessors=[cohere_rerank, recency_postprocessor],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recap\n",
    "\n",
    "In this notebook, we demonstrated how to build a recency-aware question-answering system using Llama-Index and Qdrant. We loaded a news dataset, created a Qdrant client, and loaded our data into a Llama-Index. We defined a recency postprocessor and a Cohere reranking postprocessor, and used these to create various query engines. We then queried these engines with various questions and compared their responses.\n",
    "\n",
    "Through this exercise, we learned that the use of recency postprocessing can lead to incorrect results if the recency information contradicts the correct answer. However, the use of a reranking postprocessor like Cohere can help to improve the accuracy of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
