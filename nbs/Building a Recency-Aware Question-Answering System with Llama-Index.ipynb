{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Building a Recency-Aware Question-Answering System with Llama-Index\n",
    "\n",
    "\n",
    "In this tutorial, we will learn how to build a question-answering system that is aware of the recency of information. This is especially important for questions whose answers may change over time, such as \"Who is the current US President?\".\n",
    "\n",
    "We will be using the llama_index package, a powerful tool for building large-scale information retrieval systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing necessary packages\n",
    "Before we start, make sure that you have the llama_index package installed. You can install it using pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install llama-index "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will also need the qdrant_client package for this tutorial. You can install it via pip as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install qdrant-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import qdrant_client\n",
    "from datetime import datetime\n",
    "from llama_index import GPTVectorStoreIndex\n",
    "\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from pathlib import Path\n",
    "from llama_index import GPTVectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
    "from llama_index.indices.postprocessor import (\n",
    "    FixedRecencyPostprocessor,\n",
    "    EmbeddingRecencyPostprocessor,\n",
    ")\n",
    "# load documents\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "Path.ls = lambda x: list(x.iterdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Documents\n",
    "\n",
    "First, let's load our documents. In this example, we will use the News Category Dataset v3. This dataset contains news articles with various fields like `headline`, `category`, `short_description`, `link`, `authors`, and date. We'll load the data, and reformat it to suit our needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"../data/News_Category_Dataset_v3.json\", \"r\") as f:\n",
    "    data = [json.loads(k) for k in f.readlines()]\n",
    "    links = [k.pop(\"link\") for k in data]\n",
    "    authors = [k.pop(\"authors\") for k in data]\n",
    "    data = [\n",
    "        {\n",
    "            \"text\": f\"{k['headline']} under the category: {k['category']}\\n {k['short_description']}\",\n",
    "            \"date\": k[\"date\"],\n",
    "        }\n",
    "        for k in data\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll write these documents to text files in a directory. Each document will be written to a text file named by its date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "write_dir = Path(\"../data/dump\").resolve()\n",
    "write_dir.mkdir(exist_ok=True, parents=True)\n",
    "for element in data:\n",
    "    file_path = write_dir / f\"{element['date']}.txt\"\n",
    "    with file_path.open(\"w\") as f:\n",
    "        f.write(element[\"text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what we want to build:\n",
    "\n",
    "This diagram shows that:\n",
    "\n",
    "1. The user issues a query to the query engine.\n",
    "2. The query engine, which has been configured with certain postprocessors, performs a search on the vector store based on the query.\n",
    "3. The query engine then postprocesses the results.\n",
    "4. The postprocessed results are then returned to the user.\n",
    "\n",
    "Each arrow represents the direction of data flow. The \"Query Engine\" box encapsulates the postprocessing step to indicate that it's a part of the query engine's function. This diagram is meant to provide a high-level understanding of the process and does not include all the details involved.\n",
    "\n",
    "\n",
    "We're using Llama Index to make the Query Engine and Qdrant for our Vector Store\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "+-----------+          +------------------+          +-----------------+\n",
    "|           |  Query   |                  |  Search  |                 |\n",
    "|  User     +--------->+  Query Engine    +--------->+     Qdrant      |\n",
    "|           |          |                  |          |   Vector Store  |\n",
    "|           |          | (with            |          |                 |\n",
    "+-----------+          | Postprocessors)  |          +-----------------+\n",
    "                       |                  |          |                 |\n",
    "                       |  +------------+  |          |                 |\n",
    "                       |  |Postprocess |  |  Top K   |                 | \n",
    "                       |  |            |  |Candidates|                 |\n",
    "                       +--|Candidates  |  +<---------+                 |\n",
    "                       |  +------------+  |                            |\n",
    "                       |                  |                            |\n",
    "+-----------+          +------------------+                            |\n",
    "|           |   Final  |                                               |\n",
    "|  User     |<---------+                                               |\n",
    "|           |  Answer  |                                               |\n",
    "+-----------+                                                          |\n",
    "                                                                       |\n",
    "+---------------------------------------------------------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Qdrant Client\n",
    "We'll be using Qdrant as our vector storage system. Qdrant is a high-performance vector database designed for storing and searching large-scale high-dimensional vectors.\n",
    "\n",
    "- You can use `:memory:` mode for fast and lightweight experiments. It does not require Qdrant to be deployed anywhere but requires `qdrant-client >= 1.1.1`.\n",
    "\n",
    "- Otherwise, set the Qdrant instance address with:\n",
    "  - `uri=\"http://<host>:<port>\"`\n",
    "\n",
    "- Set the API KEY for Qdrant Cloud:\n",
    "  - `api_key=\"<qdrant-api-key>\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = qdrant_client.QdrantClient(\n",
    "    location=\":memory:\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data into Llama-Index\n",
    "Llama-Index has a simple way to load documents from a directory. We can define a function to get the metadata from a file name, and pass this function to the `SimpleDirectoryReader` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_metadata(file_name: str):\n",
    "    \"\"\"Get file metadata.\"\"\"\n",
    "    return {\"date\": Path(file_name).stem}\n",
    "\n",
    "\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_files=write_dir.ls(), file_metadata=get_file_metadata\n",
    ").load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then parse these documents into nodes and create our QdrantVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import GPTVectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "\n",
    "# define service context (wrapper container around current classes)\n",
    "service_context = ServiceContext.from_defaults(chunk_size_limit=512)\n",
    "\n",
    "# use node parser in service context to parse into nodes\n",
    "nodes = service_context.node_parser.get_nodes_from_documents(documents)\n",
    "vector_store = QdrantVectorStore(client=client, collection_name=\"huffpostnews\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll create our `GPTVectorStoreIndex` from the documents. This operation might take some time as it's creating the index from the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.53 s, sys: 787 ms, total: 9.32 s\n",
      "Wall time: 6min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = GPTVectorStoreIndex.from_documents(documents, storage_context=storage_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Recency Postprocessors\n",
    "\n",
    "Llama-Index allows us to add postprocessors to our query engine. These postprocessors can modify the results of our queries after they are returned from the index. Here, we'll add a recency postprocessor to our query engine. This postprocessor will prioritize recent documents in the results.\n",
    "\n",
    "We'll define two types of recency postprocessors - `FixedRecencyPostprocessor` and `EmbeddingRecencyPostprocessor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "recency_postprocessor = FixedRecencyPostprocessor(service_context=service_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incorporating Reranking using Cohere\n",
    "Cohere is a powerful AI model that can help us rerank our results. We'll define a `CohereRerank` postprocessor and add it to our query engine.\n",
    "\n",
    "First, install the cohere package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install cohere"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using it in our code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_index.indices.postprocessor.cohere_rerank import CohereRerank\n",
    "\n",
    "cohere_rerank = CohereRerank(api_key=os.environ[\"COHERE_API_KEY\"], top_n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Query Engines\n",
    "We'll define three query engines for this tutorial - a recency query engine, a reranking query engine, and a combined query engine.\n",
    "\n",
    "The recency query engine uses the FixedRecencyPostprocessor, the reranking query engine uses the CohereRerank postprocessor, and the combined query engine uses both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "recency_query_engine = index.as_query_engine(\n",
    "    similarity_top_k=10,\n",
    "    node_postprocessors=[recency_postprocessor],\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "reranking_query_engine = index.as_query_engine(\n",
    "    similarity_top_k=10,\n",
    "    node_postprocessors=[cohere_rerank],\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k=10,\n",
    "    node_postprocessors=[cohere_rerank, recency_postprocessor],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying the Engine\n",
    "Finally, we can query our engine. Let's ask it \"Who is the current US President?\" and see the results from each query engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both: \n",
      "The current US President is Joe Biden.\n",
      "Recency: \n",
      "The current US President is Joe Biden.\n"
     ]
    }
   ],
   "source": [
    "question = \"Who is the current US President?\"\n",
    "\n",
    "response = query_engine.query(question)\n",
    "print(\"Both:\", response)\n",
    "response = recency_query_engine.query(question)\n",
    "print(\"Recency:\", response)\n",
    "response = reranking_query_engine.query(question)\n",
    "print(\"Reranking:\", response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
