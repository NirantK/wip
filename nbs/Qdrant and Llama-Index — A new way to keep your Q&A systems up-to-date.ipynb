{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: \n",
    "# Qdrant and Llama-Index: A new way to keep your Q&A systems up-to-date \n",
    "\n",
    "Ever found yourself frustrated with an answer engine that's stuck in the past? As our world rapidly evolves, the accuracy of information changes accordingly. Traditional models can become outdated, providing answers that were once accurate but are now obsolete. The cost of outdated knowledge can be high - misinforming users, impacting decision-making, and ultimately undermining trust in your system.\n",
    "\n",
    "That's where Qdrant and Llama-Index come in, revolutionizing the way answer engines stay relevant. These cutting-edge tools offer a fresh, timely perspective, ensuring your answer engine adapts to the relentless pace of information change. This tutorial will guide you in building an answer engine that stays up-to-the-minute. By mastering Qdrant and Llama-Index, you can transform your applications from static knowledge repositories into dynamic, adaptable knowledge machines. Whether you're a seasoned data scientist or an AI enthusiast, join us on this learning journey - the future of answer engines is here, and it's time to embrace it.\n",
    "\n",
    "## Tools\n",
    "\n",
    "We will be using the llama_index package, a powerful tool for building large-scale information retrieval systems.\n",
    "It is designed to work with language models like OpenAI's GPT-3. It also supports the use of postprocessors, which can modify the results of a query after they have been returned from the index. We demonstrate the impact of two interesting postprocessors.\n",
    "\n",
    "Qdrant is a high-performance vector database designed for storing and searching large-scale high-dimensional vectors. In the context of this notebook, we use Qdrant as our vector storage system.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before you begin, API Keys:\n",
    "\n",
    "Dependency: \n",
    "1. OpenAI keys for LLM. You can get them here: https://platform.openai.com/account/api-keys\n",
    "2. Cohere keys for Reranking. You can get get them here: https://dashboard.cohere.ai/api-keys\n",
    "\n",
    "If you'd like to use Qdrant Cloud, that needs: \n",
    "1. URL\n",
    "2. API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Optional] If you want to use the Qdrant Cloud, please get the Qdrant Cloud API Keys and URL\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def check_environment_keys():\n",
    "    \"\"\"\n",
    "    Utility Function that you've the NECESSARY Keys\n",
    "    \"\"\"\n",
    "    if os.environ.get(\"OPENAI_API_KEY\") is None:\n",
    "        raise ValueError(\"OPENAI_API_KEY cannot be None. Set the key using os.environ['OPENAI_API_KEY']='sk-xxx'\")\n",
    "    if os.environ.get(\"COHERE_API_KEY\") is None:\n",
    "        raise ValueError(\"COHERE_API_KEY cannot be None. Set the key using os.environ['COHERE_API_KEY']='xxx'\")\n",
    "    if os.environ.get(\"QDRANT_API_KEY\") is None:\n",
    "        print(\"[Optional] If you want to use the Qdrant Cloud, please get the Qdrant Cloud API Keys and URL\")\n",
    "        \n",
    "check_environment_keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing necessary packages\n",
    "Before we start, make sure that you have the `llama_index` package installed. You can install it using pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install llama-index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will also need the `qdrant_client` package for this tutorial. You can install it via pip as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U qdrant-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally: Rich to make error messages, stack traces easier to read\n",
    "# !pip install 'rich[jupyter]'\n",
    "%load_ext rich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import qdrant_client\n",
    "import datetime\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from pathlib import Path\n",
    "from llama_index import GPTVectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
    "from llama_index.indices.postprocessor import FixedRecencyPostprocessor\n",
    "\n",
    "import json\n",
    "from llama_index.indices.postprocessor.cohere_rerank import CohereRerank\n",
    "import os\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "from llama_index import SimpleDirectoryReader, ServiceContext\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "\n",
    "\n",
    "Path.ls = lambda x: list(x.iterdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Documents\n",
    "\n",
    "First, let's load our documents. In this example, we will use the News Category Dataset v3. This dataset contains news articles with various fields like `headline`, `category`, `short_description`, `link`, `authors`, and date. We'll load the data, and reformat it to suit our needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/News_Category_Dataset_v3.json\", \"r\") as f:\n",
    "    data = [json.loads(k) for k in f.readlines()]\n",
    "    links = [k.pop(\"link\") for k in data]\n",
    "    authors = [k.pop(\"authors\") for k in data]\n",
    "    data = [\n",
    "        {\n",
    "            \"text\": f\"{k['headline']} under the category: {k['category']}\\n {k['short_description']}\",\n",
    "            \"date\": k[\"date\"],\n",
    "        }\n",
    "        for k in data\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(datetime.datetime(2012, 1, 28, 0, 0), datetime.datetime(2022, 9, 23, 0, 0))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dates = [datetime.datetime.fromisoformat(element[\"date\"]) for element in data]\n",
    "min(dates), max(dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll write these documents to text files in a directory. Each document will be written to a text file named by its date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dir = Path(\"../data/dump\").resolve()\n",
    "write_dir.mkdir(exist_ok=True, parents=True)\n",
    "for element in data:\n",
    "    file_path = write_dir / f\"{element['date']}.txt\"\n",
    "    with file_path.open(\"w\") as f:\n",
    "        #         print(element[\"text\"])\n",
    "        f.write(element[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What do we want to build?\n",
    "\n",
    "This diagram shows that:\n",
    "\n",
    "1. The user issues a query to the query engine.\n",
    "2. The query engine, which has been configured with certain postprocessors, performs a search on the vector store based on the query.\n",
    "3. The query engine then postprocesses the results.\n",
    "4. The postprocessed results are then returned to the user.\n",
    "\n",
    "Each arrow represents the direction of data flow. The \"Query Engine\" box encapsulates the postprocessing step to indicate that it's a part of the query engine's function. This diagram is meant to provide a high-level understanding of the process and does not include all the details involved.\n",
    "\n",
    "\n",
    "We're using Llama Index to make the Query Engine and Qdrant for our Vector Store\n",
    "\n",
    "![](../images/SetupFocus.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Qdrant Client\n",
    "We'll be using Qdrant as our vector storage system. Qdrant is a high-performance vector database designed for storing and searching large-scale high-dimensional vectors.\n",
    "\n",
    "### Local Qdrant Server/Docker + Cloud Instructions\n",
    "- If you're running a local Qdrant instance with Docker, use `uri`:\n",
    "  - `uri=\"http://<host>:<port>\"`\n",
    "  \n",
    "Here I'll be using the cloud, so I am using the url set to my cloud instance\n",
    "\n",
    "- Set the API KEY for Qdrant Cloud:\n",
    "  - `api_key=\"<qdrant-api-key>\"`\n",
    "  - `url`\n",
    "\n",
    "### Memory\n",
    "\n",
    "- You can use `:memory:` mode for fast and lightweight experiments. It does not require Qdrant to be deployed anywhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = QdrantClient(\":memory:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data into Llama-Index\n",
    "Llama-Index has a simple way to load documents from a directory. We can define a function to get the metadata from a file name, and pass this function to the `SimpleDirectoryReader` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_metadata(file_name: str):\n",
    "    \"\"\"Get file metadata.\"\"\"\n",
    "    return {\"date\": Path(file_name).stem}\n",
    "\n",
    "\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_files=write_dir.ls(), file_metadata=get_file_metadata\n",
    ").load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `date` key is *necessary* for the Recency Postprocessor that we'll use later.\n",
    "\n",
    "We then parse these documents into nodes and create our QdrantVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# define service context (wrapper container around current classes)\n",
    "service_context = ServiceContext.from_defaults(chunk_size_limit=512)\n",
    "\n",
    "vector_store = QdrantVectorStore(client=client, collection_name=\"huffpostnews\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll create our `GPTVectorStoreIndex` from the documents. This operation might take some time as it's creating the index from the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.89 s, sys: 953 ms, total: 9.85 s\n",
      "Wall time: 9min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = GPTVectorStoreIndex.from_documents(documents, storage_context=storage_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making our first Query \n",
    "\n",
    "We have made an index. But as we saw in the diagram, we also need some added functionality to do 3 things:\n",
    "\n",
    "1. Retrieval\n",
    "    - Convert the text query into embedding\n",
    "    - Find the most similar documents\n",
    " \n",
    " \n",
    "2. Synthesis\n",
    "    - The LLM (here, OpenAI) texts the question, similar documents and a prompt to give you an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(similarity_top_k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The US President is Joe Biden.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"Who is the US President?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Arsenal won the English Premier League in 2013.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"Who won the EPL?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This answer is wrong and outdated: Manchester United F.C. won the 2013 club. But why is this?  \n",
    "\n",
    "This happens often because the embedding similarity can pay more attention to EPL, Arsenal than time. \n",
    "\n",
    "Let's dig a bit deeper and see the top 5 results:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 results\n",
      "Score: 0.8131673462272293. Sentence: Manchester United Dumped From FA Cup By Swansea City's Wilfried Bony (VIDEO/PHOTOS) under the category: SPORTS\n",
      " United was made to pay soon after, when Alejandro Pozuelo played in Routledge, who produced a delightful lob over Lindegaard\n",
      "Score: 0.8116888515410747. Sentence: Manchester United Defeat Liverpool, 2-1: Robin Van Persie Penalty Decides Match (VIDEO) under the category: SPORTS\n",
      " Buoyed by a rousing rendition of ‘You’ll Never Walk Alone’ in memory of the 96 football fans who died at Hillsborough 23\n",
      "Score: 0.8076903703147911. Sentence: Arsenal Routs Norwich City 4-1 To Extend Lead Atop Premier League (VIDEO/PHOTOS) under the category: SPORTS\n",
      " STORY CONTINUES BELOW Follow GOAL.COM on Twitter. Norwich continued to press forward after the interval as Snodgrass and\n",
      "Score: 0.8047559328099497. Sentence: Arsenal Eges Tottenham 1-0: Olivier Giroud Scores Only Goal In North London Derby (VIDEO/PHOTOS) under the category: SPORTS\n",
      " Villas-Boas will have been disappointed by his team's performance at both ends in the first half, with Andros Townsend the\n",
      "Score: 0.8002805600133939. Sentence: Mighty Underdogs of the European Chess Club Cup under the category: SPORTS\n",
      " Sporting fans love long shots and unexpected triumphs. The first and the last European Chess Club Cups provide proof.\n",
      "Score: 0.7962569978783269. Sentence: Luis Suarez Free Kick Goal: Liverpool Striker Nets Ridiculous Score Against Everton (GIF) under the category: SPORTS\n",
      " Liverpool striker Luis Suarez scored an unbelievable goal on a set piece that eluded Everton keeper Tim Howard on Saturday\n",
      "Score: 0.7892070675283833. Sentence: Understanding Scotland's Historic Election Result under the category: WORLDPOST\n",
      " The reality is that Scottish voters chose something markedly different than their English neighbors. But since more than 80 percent of UK voters live in England, the reality is that the UK will once again get the government that England votes for.\n",
      "Score: 0.7785890735314477. Sentence: Las Vegas Aces Win First WNBA Title, Chelsea Gray Named MVP under the category: SPORTS\n",
      " Las Vegas never had a professional sports champion — until Sunday.\n",
      "Score: 0.7785568378122035. Sentence: Juventus Wins Series A Title: Arturo Vidal Goal Seals Win Over Palermo, Back-To-Back Titles (VIDEO) under the category: SPORTS\n",
      " That near miss failed to lift the Bianconeri, though, and a wayward header from Pogba was the best they could muster before\n",
      "Score: 0.7781417330677037. Sentence: U.S. Soccer Tops Antigua and Barbuda, 3-1: Clint Dempsey, Herculez Gomez Score Goals under the category: SPORTS\n",
      " When Carlos Bocanegra bundled home a rebound from a corner seven minutes in, it looked like clear sailing for the USA against\n"
     ]
    }
   ],
   "source": [
    "k = 10\n",
    "print(f\"Top {k} results\")\n",
    "\n",
    "selected_nodes = response.source_nodes[:k]\n",
    "\n",
    "for nodewithscore in selected_nodes:\n",
    "    print(f\"Score: {nodewithscore.score}. Sentence: {nodewithscore.node.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that the Retrieval was indeed right with Manchester United being mentioned in the first few responses. Even with that Arsenal gets selected during the Synthesis step by LLM. \n",
    "Llama Index has powerful ways to compose Retrieval and Ranking steps. \n",
    "\n",
    "# Adding Postprocessor\n",
    "\n",
    "The intention behind this is to improve answer quality. Let's see if we can use Postprocessors to improve answer quality by using two approaches: \n",
    "1. Selecting the most recent node\n",
    "2. Reranking using a different model, here Cohere Reranker\n",
    "\n",
    "![](../images/RankFocus.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Recency Postprocessors\n",
    "\n",
    "Llama-Index allows us to add postprocessors to our query engine. These postprocessors can modify the results of our queries after they are returned from the index. Here, we'll add a recency postprocessor to our query engine. This postprocessor will prioritize recent documents in the results.\n",
    "\n",
    "We'll define a single type of recency postprocessors - `FixedRecencyPostprocessor`.\n",
    "\n",
    "Llama Index also has a `EmbeddingRecencyPostprocessor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "recency_postprocessor = FixedRecencyPostprocessor(service_context=service_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incorporating Reranking using Cohere\n",
    "Cohere is a powerful AI model that can help us rerank our results. We'll define a `CohereRerank` postprocessor and add it to our query engine. This can improve the search results since it's working on a smaller dataset. \n",
    "\n",
    "First, install the cohere package. You will also need a [Cohere API Key](https://docs.cohere.com/reference/key) and add to your environment. One of the ways to do can do so is this:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install cohere"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using it in our code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Query Engines\n",
    "We'll define four query engines for this tutorial: \n",
    "1. Just the Vector Store i.e. Qdrant here\n",
    "1. A recency query engine\n",
    "1. A reranking query engine\n",
    "1. And a combined query engine.\n",
    "\n",
    "The recency query engine uses the `FixedRecencyPostprocessor`, the reranking query engine uses the `CohereRerank` postprocessor, and the combined query engine uses both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 10  # set one, reuse from now on, ensures consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_query_engine = index.as_query_engine(\n",
    "    similarity_top_k=top_k,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "recency_query_engine = index.as_query_engine(\n",
    "    similarity_top_k=top_k,\n",
    "    node_postprocessors=[recency_postprocessor],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohere_rerank = CohereRerank(api_key=os.environ[\"COHERE_API_KEY\"], top_n=top_k)\n",
    "reranking_query_engine = index.as_query_engine(\n",
    "    similarity_top_k=top_k,\n",
    "    node_postprocessors=[cohere_rerank],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k=top_k,\n",
    "    node_postprocessors=[cohere_rerank, recency_postprocessor],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying the Engine\n",
    "Finally, we can query our engine. Let's ask it \"Who is the current US President?\" and see the results from each query engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The current US President is Joe Biden.\n"
     ]
    }
   ],
   "source": [
    "question = \"Who is the current US President?\"\n",
    "response = index_query_engine.query(question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `response` object has a few interesting attributes which help us quickly debug and understand what happened in each of our steps: \n",
    "1. What source nodes (similar to Document Chunks in Langchain) were used to answer the question\n",
    "2. What `extra_info` does the index have which we can use? This could also be sent as a payload to Qdrant to filter on (via epoch time) -- but Llama Index does not\n",
    "\n",
    "Let's unpack that a bit, and we'll use what we learn from `response` to improve our understanding of the query engines and post processors themselves. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `10` which is the topk parameter we set. This confirms that we got back the 10 documents most similar to the question (or more correct: 10 nearest neighbours to the question) and a confidence score. \n",
    "\n",
    "This can be made more human friendly perhaps?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Source (Doc id: 2c0719d1-219f-4fe0-b182-60917d44c718): date: 2021-06-13\n",
      "\n",
      "President Biden Meets With Queen Elizabeth At Windsor Castle under the category...\n",
      "\n",
      "> Source (Doc id: dc52cea3-6075-4375-a760-3c12f3c113ee): date: 2016-11-13\n",
      "\n",
      "Public Diplomacy in the Trump Era under the category: WORLDPOST\n",
      " ...\n",
      "\n",
      "> Source (Doc \n"
     ]
    }
   ],
   "source": [
    "print(response.get_formatted_sources()[:318])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check what is stored in the `extra_info` attribute. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2c0719d1-219f-4fe0-b182-60917d44c718': {'date': '2021-06-13'},\n",
       " 'dc52cea3-6075-4375-a760-3c12f3c113ee': {'date': '2016-11-13'},\n",
       " 'ff3e371a-0eea-4ff9-b479-7459e33b863a': {'date': '2014-08-02'},\n",
       " '5db1926f-44d8-4c8b-aba7-f1d7b80810b4': {'date': '2017-11-26'},\n",
       " 'ceb3b87f-2d85-4898-8a46-56475b9707ed': {'date': '2015-02-22'},\n",
       " 'c4e37110-028e-4fba-bf9f-3ea7ccff2e52': {'date': '2017-05-14'},\n",
       " '5ae2d94d-a97f-48b2-85e3-f4db9ea75123': {'date': '2018-11-10'},\n",
       " 'e89ddb11-2693-4ce3-ba72-ce6798402a86': {'date': '2016-05-03'},\n",
       " '0d6c797a-285c-4d99-8739-b497270df691': {'date': '2016-10-03'},\n",
       " '29e5beeb-efcb-4664-aeda-ce4b1f51682e': {'date': '2016-06-05'}}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.extra_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has a `date` key-value as a string against the `doc id`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's setup some tools to have a question, answer and the responses from the index engine in the same object - this will come handy in a bit for explaining a wrong answer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAInfo:\n",
    "    \"\"\"This class is used to store the question, correct answer and responses from different query engines.\"\"\"\n",
    "\n",
    "    def __init__(self, question: str, correct_answer: str):\n",
    "        self.question = question\n",
    "        self.correct_answer = correct_answer\n",
    "        self.responses = {}\n",
    "\n",
    "    def add_response(self, engine: str, response: str):\n",
    "        # This method is used to add the response of a query engine to the responses dictionary.\n",
    "        self.responses[engine] = response\n",
    "\n",
    "\n",
    "def compare_responses(qa_info: QAInfo, query_engines: dict):\n",
    "    \"\"\"This function takes in a QAInfo object and a dictionary of query engines, and runs the question through each query engine.\n",
    "    The responses from each engine are added to the QAInfo object.\"\"\"\n",
    "    print(\"*\" * len(qa_info.question))\n",
    "    print(f\"Question: {qa_info.question}\")\n",
    "    print(\"*\" * len(qa_info.question))\n",
    "\n",
    "    for engine_name, engine in query_engines.items():\n",
    "        response = engine.query(qa_info.question)\n",
    "        qa_info.add_response(engine_name, response)\n",
    "        print(f\"{engine_name}:\", response)\n",
    "\n",
    "    return qa_info\n",
    "\n",
    "\n",
    "query_engines = {\n",
    "    \"qdrant\": index_query_engine,\n",
    "    \"recency\": recency_query_engine,\n",
    "    \"reranking\": reranking_query_engine,\n",
    "    \"both\": query_engine,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********************\n",
      "Question: What is the gas price?\n",
      "**********************\n",
      "qdrant: \n",
      "The gas price is not specified in the context information.\n",
      "recency: \n",
      "The gas price is just below $4.\n",
      "reranking: \n",
      "The gas price is not specified in the context information.\n",
      "both: \n",
      "The gas price is just below $4.\n"
     ]
    }
   ],
   "source": [
    "question = \"What is the gas price?\"\n",
    "correct_answer = \"Less than $4 in Aug 22\"  # This would normally be determined programmatically.\n",
    "qa_info = QAInfo(question, correct_answer)\n",
    "qa_info = compare_responses(qa_info, query_engines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the Recency component plays a key role in getting this right!\n",
    "\n",
    "That looks promising. Let's try this with a question which has a specific year in it: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Question: Who was the Indian Prime Minister in January 2014?\n",
      "**************************************************\n",
      "qdrant: \n",
      "The Indian Prime Minister in January 2014 was Manmohan Singh.\n",
      "recency: \n",
      "The Indian Prime Minister in January 2014 was Narendra Modi.\n",
      "reranking: \n",
      "The Indian Prime Minister in January 2014 was Manmohan Singh.\n",
      "both: \n",
      "The Indian Prime Minister in January 2014 was Narendra Modi.\n",
      "Correct answer is Manmohan Singh\n"
     ]
    }
   ],
   "source": [
    "question = \"Who was the Indian Prime Minister in January 2014?\"\n",
    "correct_answer = \"Manmohan Singh\"  # This would normally be determined programmatically.\n",
    "qa_info = QAInfo(question, correct_answer)\n",
    "qa_info = compare_responses(qa_info, query_engines)\n",
    "print(\"Correct answer is\", qa_info.correct_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation\n",
    "\n",
    "In this question: \"Who is the Indian Prime Minister in January 2014?\", the Qdrant and reranking engines correctly respond with \"Manmohan Singh\", while the recency and combined engines incorrectly respond with \"Narendra Modi\". \n",
    "\n",
    "This demonstrates how the use of recency postprocessing can lead to incorrect results if the recency information contradicts the correct answer. Let's try another example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************\n",
      "Question: Who is the EPL Champion?\n",
      "************************\n",
      "qdrant: \n",
      "It is not possible to answer this question with the given context information.\n",
      "recency: \n",
      "The EPL Champion is not specified in the context information provided.\n",
      "reranking: \n",
      "The EPL Champion is not specified in the context information.\n",
      "both: \n",
      "The EPL Champion is not specified in the context information provided.\n",
      "Correct answer is Manchester City\n"
     ]
    }
   ],
   "source": [
    "question = \"Who is the EPL Champion?\"\n",
    "correct_answer = \"Manchester City\"  # This would normally be determined programmatically.\n",
    "qa_info = QAInfo(question, correct_answer)\n",
    "qa_info = compare_responses(qa_info, query_engines)\n",
    "print(\"Correct answer is\", qa_info.correct_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mighty Underdogs of the European Chess Club Cup under the category: SPORTS\\n Sporting fans love long shots and unexpected triumphs. The first and the last European Chess Club Cups provide proof.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_info.responses[\"qdrant\"].source_nodes[0].node.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Manchester United Defeat Liverpool, 2-1: Robin Van Persie Penalty Decides Match (VIDEO) under the category: SPORTS\\n Buoyed by a rousing rendition of ‘You’ll Never Walk Alone’ in memory of the 96 football fans who died at Hillsborough 23'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_info.responses[\"reranking\"].source_nodes[0].node.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Las Vegas Aces Win First WNBA Title, Chelsea Gray Named MVP under the category: SPORTS\\n Las Vegas never had a professional sports champion — until Sunday.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_info.responses[\"recency\"].source_nodes[0].node.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Las Vegas Aces Win First WNBA Title, Chelsea Gray Named MVP under the category: SPORTS\\n Las Vegas never had a professional sports champion — until Sunday.'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_info.responses[\"both\"].source_nodes[0].node.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation\n",
    "\n",
    "We notice that receny did have a node which selected a node with the correct answer (\"Manchester United\"). The synthesis step ignored that and contributed to our quest being unfulfilled. Maybe we'll take another stab at this in a future tutorial perhaps!\n",
    "\n",
    "\n",
    "# Recap\n",
    "\n",
    "In this notebook, we demonstrated how to build a recency-aware question-answering system using Llama-Index and Qdrant. We loaded a news dataset, created a Qdrant client, and loaded our data into a Llama-Index. We defined a recency postprocessor and a Cohere reranking postprocessor, and used these to create various query engines. We then queried these engines with various questions and compared their responses.\n",
    "\n",
    "Through this exercise, we learned that the use of recency postprocessing can lead to incorrect results if the recency information contradicts the correct answer. However, the use of a reranking postprocessor like Cohere can help to improve the accuracy of the results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
