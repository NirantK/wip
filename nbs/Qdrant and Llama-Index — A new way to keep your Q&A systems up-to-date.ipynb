{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tutorial\n",
    "---\n",
    "# Qdrant and Llama-Index: A new way to keep your Q&A systems up-to-date \n",
    "\n",
    "Ever found yourself frustrated with an answer engine that's stuck in the past? As our world rapidly evolves, the accuracy of information changes accordingly. Traditional models can become outdated, providing answers that were once accurate but are now obsolete. The cost of outdated knowledge can be high - misinforming users, impacting decision-making, and ultimately undermining trust in your system.\n",
    "\n",
    "That's where Qdrant and Llama-Index come in, ensuring that answer engines stay relevant. These cutting-edge tools offer a fresh, timely perspective, ensuring your answer engine adapts to the relentless pace of information change.\n",
    "\n",
    "This tutorial will demonstrate **how to build an answer engine that stays up-to-the-minute**. \n",
    "\n",
    "By mastering Qdrant and Llama-Index, you can transform your applications from static knowledge repositories into dynamic, adaptable knowledge machines. Whether you're a seasoned data scientist or an AI enthusiast, join us on this learning journey - the future of answer engines is here, and it's time to embrace it.\n",
    "\n",
    "The tutorial covers setting up the necessary tools and API keys, loading documents, and adding postprocessors to improve the relevance of search results. The News Category Dataset v3 is used as an example dataset.\n",
    "\n",
    "# Table of Contents\n",
    "\n",
    "- Setting Up\n",
    "    - Tools\n",
    "    - Before you begin, API Keys\n",
    "    - Installing & Importing necessary packages\n",
    "- Loading Documents\n",
    "  - What do we want to build?\n",
    "  - Creating Qdrant Client\n",
    "    - Local Qdrant Server/Docker + Cloud Instructions\n",
    "    - Memory\n",
    "  - Loading Data into Llama-Index\n",
    "  - Making our first Query\n",
    "- Adding Postprocessor\n",
    "  - Defining Recency Postprocessors\n",
    "  - Incorporating Reranking using Cohere\n",
    "  - Defining Query Engines\n",
    "  - Querying the Engine\n",
    "    - Observation\n",
    "    - Observation\n",
    "- Recap\n",
    "\n",
    "## Setting Up\n",
    "\n",
    "### Tools\n",
    "\n",
    "We will be using: \n",
    "1. `llama_index` package, a powerful tool for building large-scale information retrieval systems. If this is the first time you're coming across this library, [get started here](https://gpt-index.readthedocs.io/en/latest/getting_started/starter_example.html) \n",
    "2. `Qdrant` is a high-performance vector database designed for storing and searching large-scale high-dimensional vectors. In the context of this notebook, we use Qdrant as our vector storage system.\n",
    "3. `cohere` is a Reranking service: takes in a query and a list of texts and returns an ordered array with each text assigned a _new_ relevance score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before you begin, API Keys:\n",
    "\n",
    "Dependency: \n",
    "1. OpenAI keys for LLM. You can get them here: https://platform.openai.com/account/api-keys\n",
    "2. Cohere keys for Reranking. You can get get them here: https://dashboard.cohere.ai/api-keys and some more docs on it: [Cohere API Key](https://docs.cohere.com/reference/key) \n",
    "\n",
    "If you'd like to use Qdrant Cloud, you can get the following from here: https://cloud.qdrant.io/ -> Access: \n",
    "1. URL\n",
    "2. API Keys\n",
    "\n",
    "The tutorial is written to work with the Qdrant client without depending on the cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Optional] If you want to use the Qdrant Cloud, please get the Qdrant Cloud API Keys and URL\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def check_environment_keys():\n",
    "    \"\"\"\n",
    "    Utility Function that you've the NECESSARY Keys\n",
    "    \"\"\"\n",
    "    if os.environ.get(\"OPENAI_API_KEY\") is None:\n",
    "        raise ValueError(\n",
    "            \"OPENAI_API_KEY cannot be None. Set the key using os.environ['OPENAI_API_KEY']='sk-xxx'\"\n",
    "        )\n",
    "    if os.environ.get(\"COHERE_API_KEY\") is None:\n",
    "        raise ValueError(\n",
    "            \"COHERE_API_KEY cannot be None. Set the key using os.environ['COHERE_API_KEY']='xxx'\"\n",
    "        )\n",
    "    if os.environ.get(\"QDRANT_API_KEY\") is None:\n",
    "        print(\"[Optional] If you want to use the Qdrant Cloud, please get the Qdrant Cloud API Keys and URL\")\n",
    "\n",
    "\n",
    "check_environment_keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing & Importing necessary packages\n",
    "Before we start, make sure that you have the `llama_index` package installed. For post processing, we will Cohere's Rerank API. You can install both using pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install llama-index\n",
    "# !pip install cohere\n",
    "# !pip install datasets\n",
    "# !pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will also need the `qdrant_client` package for this tutorial. You can install it via pip as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U qdrant-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally: Rich to make error messages, stack traces easier to read\n",
    "# !pip install 'rich[jupyter]'\n",
    "%load_ext rich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from llama_index import (ServiceContext,\n",
    "                         SimpleDirectoryReader, StorageContext,\n",
    "                         GPTVectorStoreIndex)\n",
    "from llama_index.indices.postprocessor import FixedRecencyPostprocessor\n",
    "from llama_index.indices.postprocessor.cohere_rerank import CohereRerank\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "Path.ls = lambda x: list(x.iterdir())\n",
    "random.seed(42)  # This is the answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What do we want to build?\n",
    "![](../images/SetupFocus.png)\n",
    "\n",
    "\n",
    "We will build an answer engine which stays updated. This has two main parts:\n",
    "1. Retrieval - Done with Qdrant\n",
    "2. Synthesis - Done with OpenAI API\n",
    "\n",
    "Later, we'll add our components which helps us keep this updated and improve ranking after retrieval\n",
    "\n",
    "Each arrow represents the direction of data flow. The \"Query Engine\" box encapsulates the postprocessing step to indicate that it's a part of the query engine's function. This diagram is meant to provide a high-level understanding of the process and does not include all the details involved.\n",
    "\n",
    "\n",
    "We're using Llama Index to make the Query Engine and Qdrant for our Vector Store\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Documents\n",
    "\n",
    "First, let's load our documents. In this example, we will use the News Category Dataset v3. This dataset contains news articles with various fields like `headline`, `category`, `short_description`, `link`, `authors`, and date. We'll load the data, and reformat it to suit our needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/Users/nirantk/.cache/huggingface/datasets/heegyu___json/heegyu--news-category-dataset-a0dcb53f17af71bf/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"heegyu/news-category-dataset\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                link   \n",
      "0  https://www.huffpost.com/entry/covid-boosters-...  \\\n",
      "1  https://www.huffpost.com/entry/american-airlin...   \n",
      "2  https://www.huffpost.com/entry/funniest-tweets...   \n",
      "3  https://www.huffpost.com/entry/funniest-parent...   \n",
      "4  https://www.huffpost.com/entry/amy-cooper-lose...   \n",
      "\n",
      "                                            headline   category   \n",
      "0  Over 4 Million Americans Roll Up Sleeves For O...  U.S. NEWS  \\\n",
      "1  American Airlines Flyer Charged, Banned For Li...  U.S. NEWS   \n",
      "2  23 Of The Funniest Tweets About Cats And Dogs ...     COMEDY   \n",
      "3  The Funniest Tweets From Parents This Week (Se...  PARENTING   \n",
      "4  Woman Who Called Cops On Black Bird-Watcher Lo...  U.S. NEWS   \n",
      "\n",
      "                                   short_description               authors   \n",
      "0  Health experts said it is too early to predict...  Carla K. Johnson, AP  \\\n",
      "1  He was subdued by passengers and crew when he ...        Mary Papenfuss   \n",
      "2  \"Until you have a dog you don't understand wha...         Elyse Wanshel   \n",
      "3  \"Accidentally put grown-up toothpaste on my to...      Caroline Bologna   \n",
      "4  Amy Cooper accused investment firm Franklin Te...        Nina Golgowski   \n",
      "\n",
      "        date  \n",
      "0 2022-09-23  \n",
      "1 2022-09-23  \n",
      "2 2022-09-23  \n",
      "3 2022-09-23  \n",
      "4 2022-09-22  \n"
     ]
    }
   ],
   "source": [
    "def get_single_text(k):\n",
    "    return f\"Under the category:\\n{k['category']}:\\n{k['headline']}\\n{k['short_description']}\"\n",
    "\n",
    "\n",
    "df = pd.DataFrame(dataset)\n",
    "print(df.head())\n",
    "# print(df.category.value_counts())\n",
    "category_columns_to_keep = [\"POLITICS\", \"THE WORLDPOST\", \"WORLD NEWS\", \"WORLDPOST\", \"U.S. NEWS\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming `df` is your original dataframe\n",
    "df[\"year\"] = df[\"date\"].dt.year\n",
    "\n",
    "category_columns_to_keep = [\"POLITICS\", \"THE WORLDPOST\", \"WORLD NEWS\", \"WORLDPOST\", \"U.S. NEWS\"]\n",
    "\n",
    "# Filter by category\n",
    "df_filtered = df[df[\"category\"].isin(category_columns_to_keep)]\n",
    "\n",
    "# Sample data for each year\n",
    "\n",
    "\n",
    "def sample_func(x):\n",
    "    return x.sample(min(len(x), 200))\n",
    "\n",
    "\n",
    "df_sampled = df_filtered.groupby(\"year\").apply(sample_func).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "year\n",
       "2014    200\n",
       "2015    200\n",
       "2016    200\n",
       "2017    200\n",
       "2018    200\n",
       "2019    200\n",
       "2020    200\n",
       "2021    200\n",
       "2022    200\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sampled[\"year\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Under the category:\\nWORLDPOST:\\nPrepping for ...\n",
       "1       Under the category:\\nPOLITICS:\\nPelosi Will Hi...\n",
       "2       Under the category:\\nPOLITICS:\\nObama Spells O...\n",
       "3       Under the category:\\nPOLITICS:\\nBill Foster De...\n",
       "4       Under the category:\\nPOLITICS:\\nU.S. Opens Cri...\n",
       "                              ...                        \n",
       "1795    Under the category:\\nWORLD NEWS:\\nOperation Un...\n",
       "1796    Under the category:\\nPOLITICS:\\nChilling Trump...\n",
       "1797    Under the category:\\nU.S. NEWS:\\nMichigan Bask...\n",
       "1798    Under the category:\\nPOLITICS:\\nMark Meadows S...\n",
       "1799    Under the category:\\nWORLD NEWS:\\nBiden At UN ...\n",
       "Name: text, Length: 1800, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"text\"] = df.apply(get_single_text, axis=1)\n",
    "df[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Under the category:\\nPOLITICS:\\nHow Will Medicaid Directors Handle the Boom in Applications From the Affordable Care Act?\\nThe second open enrollment will be heavily focused on bringing in even harder to reach populations, many of whom will be deemed eligible for Medicaid coverage.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"text\"][9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=[\"year\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll write these documents to text files in a directory. Each document will be written to a text file named by its date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 58.1 ms, sys: 110 ms, total: 169 ms\n",
      "Wall time: 167 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "write_dir = Path(\"../data/sample\").resolve()\n",
    "if write_dir.exists():\n",
    "    [f.unlink() for f in write_dir.ls()]\n",
    "write_dir.mkdir(exist_ok=True, parents=True)\n",
    "for index, row in df.iterrows():\n",
    "    date = str(row[\"date\"]).replace(\"-\", \"_\")  # replace '-' in date with '_' to avoid issues with file names\n",
    "    file_path = write_dir / f\"date_{date}_row_{index}.txt\"\n",
    "    with file_path.open(\"w\") as f:\n",
    "        f.write(row[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del dataset, df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Qdrant Client\n",
    "We'll be using Qdrant as our vector storage system. Qdrant is a high-performance vector database designed for storing and searching large-scale high-dimensional vectors.\n",
    "\n",
    "### Local Qdrant Server/Docker + Cloud Instructions\n",
    "- If you're running a local Qdrant instance with Docker, use `uri`:\n",
    "  - `uri=\"http://<host>:<port>\"`\n",
    "  \n",
    "Here I'll be using the cloud, so I am using the url set to my cloud instance\n",
    "\n",
    "- Set the API KEY for Qdrant Cloud:\n",
    "  - `api_key=\"<qdrant-api-key>\"`\n",
    "  - `url`\n",
    "\n",
    "### Memory\n",
    "\n",
    "- You can use `:memory:` mode for fast and lightweight experiments. It does not require Qdrant to be deployed anywhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = QdrantClient(\n",
    "    url=\"https://1a02d8e1-d527-4a48-8719-81ce17cda1c3.us-east-1-0.aws.cloud.qdrant.io:6333\",\n",
    "    api_key=\"Ny_D6kwbz02QQ1CDYUGdCSx_UGCU1TmKFsV-Rr51zTynG4DIZuSpXw\",\n",
    "    prefer_grpc=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data into Llama-Index\n",
    "Llama-Index has a simple way to load documents from a directory. We can define a function to get the metadata from a file name, and pass this function to the `SimpleDirectoryReader` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_metadata(file_name: str):\n",
    "    \"\"\"Get file metadata.\"\"\"\n",
    "    date_str = Path(file_name).stem.split(\"_\")[1:4]\n",
    "    return {\"date\": \"-\".join(date_str)}\n",
    "\n",
    "\n",
    "documents = SimpleDirectoryReader(input_files=write_dir.ls(), file_metadata=get_file_metadata).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1800"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(datetime.datetime(2014, 4, 18, 0, 0), datetime.datetime(2022, 9, 23, 0, 0))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dates = []\n",
    "\n",
    "for document in documents:\n",
    "    d = document.extra_info[\"date\"]\n",
    "    #     print(d)\n",
    "    try:\n",
    "        dates.append(datetime.datetime.fromisoformat(d))\n",
    "    except:\n",
    "        print(d)\n",
    "\n",
    "min(dates), max(dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `date` key is *necessary* for the Recency Postprocessor that we'll use later.\n",
    "\n",
    "We then parse these documents into nodes and create our QdrantVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# define service context (wrapper container around current classes)\n",
    "service_context = ServiceContext.from_defaults(chunk_size_limit=512)\n",
    "vector_store = QdrantVectorStore(client=client, collection_name=\"NewsCategoryv3PoliticsSample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll create our `GPTVectorStoreIndex` from the documents. This operation might take some time as it's creating the index from the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = GPTVectorStoreIndex.from_documents(documents, vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making our first Query \n",
    "\n",
    "We have made an index. But as we saw in the diagram, we also need some added functionality to do 3 things:\n",
    "\n",
    "1. Retrieval\n",
    "    - Convert the text query into embedding\n",
    "    - Find the most similar documents\n",
    " \n",
    " \n",
    "2. Synthesis\n",
    "    - The LLM (here, OpenAI) texts the question, similar documents and a prompt to give you an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(similarity_top_k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The US President is Barack Obama.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"Who is the US President?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The current US President is Donald Trump.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"Who is the current US President?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response.source_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The EPL (English Premier League) does not have a winner yet for the given dates.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"Who won the EPL?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Adding Postprocessor\n",
    "\n",
    "Llama Index has powerful ways to compose Retrieval and Ranking steps. \n",
    "\n",
    "The intention behind this is to improve answer quality. Let's see if we can use Postprocessors to improve answer quality by using two approaches: \n",
    "1. Selecting the most recent nodes (i.e. sort by recency)\n",
    "2. Reranking using a different model, here Cohere Reranker\n",
    "\n",
    "![](../images/RankFocus.png)\n",
    "\n",
    "Here is what the diagram represents:\n",
    "1. The user issues a query to the query engine.\n",
    "2. The query engine, which has been configured with certain postprocessors, performs a search on the vector store based on the query.\n",
    "3. The query engine then postprocesses the results.\n",
    "4. The postprocessed results are then returned to the user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Recency Postprocessors\n",
    "\n",
    "Llama-Index allows us to add postprocessors to our query engine. These postprocessors can modify the results of our queries after they are returned from the index. Here, we'll add a recency postprocessor to our query engine. This postprocessor will prioritize recent documents in the results.\n",
    "\n",
    "We'll define a single type of recency postprocessor: `FixedRecencyPostprocessor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "recency_postprocessor = FixedRecencyPostprocessor(service_context=service_context, top_k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cohere Rerank\n",
    "\n",
    "Cohere Rerank works on the top K results which the Retrieval step from Qdrant returns. While Qdrant works on your entire corpus (here thousands, but Qdrant is designed to work with millions) -- Cohere works with the result from Qdrant. This can improve the search results since it's working on smaller number of entries. \n",
    "\n",
    "![](../images/RerankFocus.png)\n",
    "\n",
    "\n",
    "Rerank endpoint takes in a query and a list of texts and produces an ordered array with each text assigned a relevance score. We'll define a `CohereRerank` postprocessor and add it to our query engine. T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Query Engines\n",
    "We'll define four query engines for this tutorial: \n",
    "1. Just the Vector Store i.e. Qdrant here\n",
    "1. A recency query engine\n",
    "1. A reranking query engine\n",
    "1. And a combined query engine.\n",
    "\n",
    "The recency query engine uses the `FixedRecencyPostprocessor`, the reranking query engine uses the `CohereRerank` postprocessor, and the combined query engine uses both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 10  # set one, reuse from now on, ensures consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_query_engine = index.as_query_engine(\n",
    "    similarity_top_k=top_k,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "recency_query_engine = index.as_query_engine(\n",
    "    similarity_top_k=top_k,\n",
    "    node_postprocessors=[recency_postprocessor],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohere_rerank = CohereRerank(api_key=os.environ[\"COHERE_API_KEY\"], top_n=top_k)\n",
    "reranking_query_engine = index.as_query_engine(\n",
    "    similarity_top_k=top_k,\n",
    "    node_postprocessors=[cohere_rerank],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k=top_k,\n",
    "    node_postprocessors=[cohere_rerank, recency_postprocessor],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying the Engine\n",
    "Finally, we can query our engine. Let's ask it \"Who is the current US President?\" and see the results from each query engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The US President is Barack Obama.\n"
     ]
    }
   ],
   "source": [
    "# question = \"Who is the current US President?\"\n",
    "response = index_query_engine.query(\"Who is the US President?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `response` object has a few interesting attributes which help us quickly debug and understand what happened in each of our steps: \n",
    "1. What source nodes (similar to Document Chunks in Langchain) were used to answer the question\n",
    "2. What `extra_info` does the index have which we can use? This could also be sent as a payload to Qdrant to filter on (via epoch time) -- but Llama Index does not\n",
    "\n",
    "Let's unpack that a bit, and we'll use what we learn from `response` to improve our understanding of the query engines and post processors themselves. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `10` which is the topk parameter we set. This confirms that we got back the 10 documents most similar to the question (or more correct: 10 nearest neighbours to the question) and a confidence score. \n",
    "\n",
    "This can be made more human friendly perhaps?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Source (Doc id: dd85b761-f599-47ab-bed6-feafe8ea2094): date: 2016-08-21 00:00:00\n",
      "\n",
      "Under the category:\n",
      "POLITICS:\n",
      "The Five Reasons Donald Trump Will Be Ou...\n",
      "\n",
      "> Source (Doc id: 482e1da2-3e78-4bdd-b1f3-11c74c34ef1a): date: 2014-11-08 00:00:00\n",
      "\n",
      "Under the category:\n",
      "POLITICS:\n",
      "Global Leadership for U.S. National Secu...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(response.get_formatted_sources()[:318])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check what is stored in the `extra_info` attribute. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dd85b761-f599-47ab-bed6-feafe8ea2094': {'date': '2016-08-21 00:00:00'},\n",
       " '482e1da2-3e78-4bdd-b1f3-11c74c34ef1a': {'date': '2014-11-08 00:00:00'},\n",
       " '429221c0-8a2a-4e3d-a46b-4b6760650ae9': {'date': '2015-12-31 00:00:00'},\n",
       " 'a5c5dd77-d13d-4251-974f-339fe10d49d7': {'date': '2016-02-08 00:00:00'},\n",
       " '51d02bb9-cdab-43b2-a226-5e51869a23e6': {'date': '2014-05-03 00:00:00'},\n",
       " '095227ed-79f4-49dd-b2a6-d35e81551faa': {'date': '2015-03-15 00:00:00'},\n",
       " 'bd9a3880-b675-49f6-84b9-2b14531e1295': {'date': '2018-01-24 00:00:00'},\n",
       " '9ab19555-0bcf-4e4b-9579-e4dd0262693a': {'date': '2014-10-05 00:00:00'},\n",
       " '01a7c6e9-f62d-4271-9a73-716615ad9ae7': {'date': '2016-04-09 00:00:00'},\n",
       " '9bb9b689-643b-4745-9572-5a5f3ae5311d': {'date': '2016-03-07 00:00:00'}}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.extra_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has a `date` key-value as a string against the `doc id`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's setup some tools to have a question, answer and the responses from the index engine in the same object - this will come handy in a bit for explaining a wrong answer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display_markdown\n",
    "\n",
    "def mprint(text: str):\n",
    "    display_markdown(Markdown(text))\n",
    "\n",
    "class QAInfo:\n",
    "    \"\"\"This class is used to store the question, correct answer and responses from different query engines.\"\"\"\n",
    "\n",
    "    def __init__(self, question: str, correct_answer: str, query_engines: dict[str, Any]):\n",
    "        self.question = question\n",
    "        self.query_engines = query_engines\n",
    "        self.correct_answer = correct_answer\n",
    "        self.responses = {}\n",
    "\n",
    "    def add_response(self, engine: str, response: str):\n",
    "        # This method is used to add the response of a query engine to the responses dictionary.\n",
    "        self.responses[engine] = response\n",
    "\n",
    "\n",
    "    def compare_responses(self):\n",
    "        \n",
    "        \n",
    "        \"\"\"This function takes in a QAInfo object and a dictionary of query engines, and runs the question through each query engine.\n",
    "        The responses from each engine are added to the QAInfo object.\"\"\"\n",
    "        mprint(f\"### Question: {self.question}\")\n",
    "\n",
    "        for engine_name, engine in query_engines.items():\n",
    "            response = engine.query(self.question)\n",
    "            self.add_response(engine_name, response)\n",
    "            mprint(f\"**{engine_name.title()}**: {response}\")\n",
    "\n",
    "        mprint(f\"Correct Answer is: {self.correct_answer}\")\n",
    "    \n",
    "    def node_print(self, index, preview_count=5):\n",
    "        source_nodes = self.responses[index].source_nodes\n",
    "        for i in range(preview_count):\n",
    "            mprint(f\"- {source_nodes[i].node.text}\")\n",
    "            \n",
    "\n",
    "\n",
    "query_engines = {\n",
    "    \"qdrant\": index_query_engine,\n",
    "    \"recency\": recency_query_engine,\n",
    "    \"reranking\": reranking_query_engine,\n",
    "    \"both\": query_engine,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Question: Who is the US President?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Qdrant**: \n",
       "The US President is Barack Obama."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Recency**: \n",
       "Donald Trump."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Reranking**: \n",
       "The US President is Barack Obama."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Both**: \n",
       "Donald Trump."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Correct Answer is: Joe Biden"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"Who is the US President?\"\n",
    "correct_answer = \"Joe Biden\"  # This would normally be determined programmatically.\n",
    "president_qa_info = QAInfo(question=question, correct_answer=correct_answer, query_engines=query_engines)\n",
    "president_qa_info.compare_responses()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impact of how a question is asked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Question: Who is current US President?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Qdrant**: \n",
       "The current US President is Joe Biden."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Recency**: \n",
       "The current US President is Donald Trump."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Reranking**: \n",
       "The current US President is Joe Biden."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Both**: \n",
       "Donald Trump."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Correct Answer is: Joe Biden"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"Who is current US President?\"\n",
    "correct_answer = \"Joe Biden\"  # This would normally be determined programmatically.\n",
    "current_president_qa_info = QAInfo(question=question, correct_answer=correct_answer, query_engines=query_engines)\n",
    "current_president_qa_info.compare_responses()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a specific Year\n",
    "\n",
    "That looks interesting. Let's try this with a question which has a specific year in it: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Question: Who was the US President in 2010?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Qdrant**: \n",
       "The US President in 2010 was Barack Obama."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Recency**: \n",
       "The US President in 2010 was Barack Obama."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Reranking**: \n",
       "The US President in 2010 was Barack Obama."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Both**: \n",
       "The US President in 2010 was Barack Obama."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Correct Answer is: Barack Obama"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"Who was the US President in 2010?\"\n",
    "correct_answer = \"Barack Obama\"  # This would normally be determined programmatically.\n",
    "president_2010_qa_info = QAInfo(question=question, correct_answer=correct_answer, query_engines=query_engines)\n",
    "president_2010_qa_info.compare_responses()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a different variant of this question, specify a year and see what happens? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Question: Who was the Indian Prime Minister in January 2014?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Qdrant**: \n",
       "The Indian Prime Minister in January 2014 was Narendra Modi."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Recency**: \n",
       "The Indian Prime Minister in January 2014 was Manmohan Singh."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Reranking**: \n",
       "The Indian Prime Minister in January 2014 was Narendra Modi."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Both**: \n",
       "The Indian Prime Minister in January 2014 was Manmohan Singh."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Correct Answer is: Manmohan Singh"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"Who was the Indian Prime Minister in January 2014?\"\n",
    "correct_answer = \"Manmohan Singh\"  # This would normally be determined programmatically.\n",
    "prime_minister_jan2014 = QAInfo(question=question, correct_answer=correct_answer, query_engines=query_engines)\n",
    "prime_minister_jan2014.compare_responses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Question: Who was the Indian Prime Minister in December 2014?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Qdrant**: \n",
       "The Indian Prime Minister in December 2014 was Narendra Modi."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Recency**: \n",
       "The Indian Prime Minister in December 2014 was Narendra Modi."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Reranking**: \n",
       "The Indian Prime Minister in December 2014 was Narendra Modi."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Both**: \n",
       "The Indian Prime Minister in December 2014 was Narendra Modi."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Correct Answer is: Narendra Modi"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"Who was the Indian Prime Minister in December 2014?\"\n",
    "correct_answer = \"Narendra Modi\"  # This would normally be determined programmatically.\n",
    "prime_minister_dec2014 = QAInfo(question=question, correct_answer=correct_answer, query_engines=query_engines)\n",
    "prime_minister_dec2014.compare_responses()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation\n",
    "\n",
    "In this question: \"Who is the Indian Prime Minister in January 2014?\", the Qdrant and reranking engines incorrectly respond with \"Narendra Modi\", while the recency and combined engines correctly respond with \"Manmohan Singh\". \n",
    "\n",
    "This demonstrates how the use of recency postprocessing can lead to incorrect results if the recency information contradicts the correct answer. Let's investigate the sources further:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "- Under the category:\n",
       "WORLDPOST:\n",
       "This Chart Shows Which World Leaders Are the Most Powerful on Twitter\n",
       "The new prime minister of India, @NarendraModi, just surpassed the @WhiteHouse in fourth place and is likely to have more followers than Indonesian President Susilo Bambang Yudhoyono next week."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "- Under the category:\n",
       "WORLD NEWS:\n",
       "After India's Amazon Snub, Modi's Party Slams Bezos-Owned Washington Post\n",
       "An official has been critical of foreign media’s reporting on political issues in the past."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "- Under the category:\n",
       "U.S. NEWS:\n",
       "Activists Petition Gates Foundation Not To Honor India's Prime Minister\n",
       "Actors Riz Ahmed and Jameela Jamil have also pulled out of the event in which an award is to be given to Narendra Modi despite India's crackdown in Kashmir."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prime_minister_dec2014.node_print(index=\"qdrant\", preview_count=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "- Under the category:\n",
       "WORLD NEWS:\n",
       "After India's Amazon Snub, Modi's Party Slams Bezos-Owned Washington Post\n",
       "An official has been critical of foreign media’s reporting on political issues in the past."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "- Under the category:\n",
       "U.S. NEWS:\n",
       "Activists Petition Gates Foundation Not To Honor India's Prime Minister\n",
       "Actors Riz Ahmed and Jameela Jamil have also pulled out of the event in which an award is to be given to Narendra Modi despite India's crackdown in Kashmir."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "- Under the category:\n",
       "WORLD NEWS:\n",
       "Israeli Police Recommend Prime Minister Netanyahu Be Indicted On Corruption Charges\n",
       "He could face indictment after a monthslong investigation into two separate allegations of corruption."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prime_minister_dec2014.node_print(index=\"recency\", preview_count=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "- Under the category:\n",
       "WORLDPOST:\n",
       "Japan's Election Sees A Landslide Victory For Ruling Party\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "- Under the category:\n",
       "WORLD NEWS:\n",
       "After India's Amazon Snub, Modi's Party Slams Bezos-Owned Washington Post\n",
       "An official has been critical of foreign media’s reporting on political issues in the past."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "- Under the category:\n",
       "WORLDPOST:\n",
       "This Chart Shows Which World Leaders Are the Most Powerful on Twitter\n",
       "The new prime minister of India, @NarendraModi, just surpassed the @WhiteHouse in fourth place and is likely to have more followers than Indonesian President Susilo Bambang Yudhoyono next week."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prime_minister_dec2014.node_print(index=\"reranking\", preview_count=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation\n",
    "\n",
    "We notice that receny did have a node which selected a node with the correct answer (\"Manchester United\"). The synthesis step ignored that and contributed to our quest being unfulfilled. Maybe we'll take another stab at this in a future tutorial perhaps!\n",
    "\n",
    "\n",
    "# Recap\n",
    "\n",
    "In this notebook, we demonstrated how to build a recency-aware question-answering system using Llama-Index and Qdrant. We loaded a news dataset, created a Qdrant client, and loaded our data into a Llama-Index. We defined a recency postprocessor and a Cohere reranking postprocessor, and used these to create various query engines. We then queried these engines with various questions and compared their responses.\n",
    "\n",
    "Through this exercise, we learned that the use of recency postprocessing can lead to incorrect results if the recency information contradicts the correct answer. However, the use of a reranking postprocessor like Cohere can help to improve the accuracy of the results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
